{
    "src": {
        "org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorMapper": {
            "method": {
                "reduce": "{\n  throw new IOException(\"should not be called\\n\");\n}\n",
                "map": "{\n  Iterator iter=this.aggregatorDescriptorList.iterator();\n  while (iter.hasNext()) {\n    ValueAggregatorDescriptor ad=(ValueAggregatorDescriptor)iter.next();\n    Iterator<Entry<Text,Text>> ens=ad.generateKeyValPairs(key,value).iterator();\n    while (ens.hasNext()) {\n      Entry<Text,Text> en=ens.next();\n      output.collect(en.getKey(),en.getValue());\n    }\n  }\n}\n"
            },
            "className": "org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorMapper",
            "body": "class ValueAggregatorMapper{public void map(K1 key,V1 value,OutputCollector<Text,Text> output,Reporter reporter) throws IOException {} public void reduce(Text arg0,Iterator<Text> arg1,OutputCollector<Text,Text> arg2,Reporter arg3) throws IOException {}  }",
            "classAnnotation": "/** \n * This class implements the generic mapper of Aggregate.\n */\n",
            "fullmethod": {
                "reduce": "/** \n * Do nothing. Should not be called.\n */\npublic void reduce(Text arg0,Iterator<Text> arg1,OutputCollector<Text,Text> arg2,Reporter arg3) throws IOException {\n  throw new IOException(\"should not be called\\n\");\n}\n",
                "map": "/** \n * the map function. It iterates through the value aggregator descriptor  list to generate aggregation id/value pairs and emit them.\n */\npublic void map(K1 key,V1 value,OutputCollector<Text,Text> output,Reporter reporter) throws IOException {\n  Iterator iter=this.aggregatorDescriptorList.iterator();\n  while (iter.hasNext()) {\n    ValueAggregatorDescriptor ad=(ValueAggregatorDescriptor)iter.next();\n    Iterator<Entry<Text,Text>> ens=ad.generateKeyValPairs(key,value).iterator();\n    while (ens.hasNext()) {\n      Entry<Text,Text> en=ens.next();\n      output.collect(en.getKey(),en.getValue());\n    }\n  }\n}\n"
            },
            "deps": [
                "org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorMapper Import org.apache.hadoop.io.WritableComparable",
                "org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorMapper Use org.apache.hadoop.io.WritableComparable",
                "org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorMapper Import org.apache.hadoop.io.Writable",
                "org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorMapper Use org.apache.hadoop.io.Writable",
                "org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorMapper Cast org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorDescriptor",
                "org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorMapper Call org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorDescriptor",
                "org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorMapper Contain org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorDescriptor",
                "org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorMapper Extend org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorJobBase",
                "org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorMapper Import org.apache.hadoop.mapred.Reporter",
                "org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorMapper Parameter org.apache.hadoop.mapred.Reporter",
                "org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorMapper Call org.apache.hadoop.mapred.OutputCollector",
                "org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorMapper Import org.apache.hadoop.mapred.OutputCollector",
                "org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorMapper Parameter org.apache.hadoop.mapred.OutputCollector",
                "org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorJob Use org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorMapper"
            ],
            "directory": "org.apache.hadoop.mapred.lib.aggregate"
        },
        "org.apache.hadoop.fs.BlockLocation": {
            "method": {
                "setHosts": "{\n  if (hosts == null) {\n    this.hosts=new String[0];\n  }\n else {\n    this.hosts=hosts;\n  }\n}\n",
                "readFields": "{\n  this.offset=in.readLong();\n  this.length=in.readLong();\n  int numNames=in.readInt();\n  this.names=new String[numNames];\n  for (int i=0; i < numNames; i++) {\n    Text name=new Text();\n    name.readFields(in);\n    names[i]=name.toString();\n  }\n  int numHosts=in.readInt();\n  for (int i=0; i < numHosts; i++) {\n    Text host=new Text();\n    host.readFields(in);\n    hosts[i]=host.toString();\n  }\n}\n",
                "setNames": "{\n  if (names == null) {\n    this.names=new String[0];\n  }\n else {\n    this.names=names;\n  }\n}\n",
                "setLength": "{\n  this.length=length;\n}\n",
                "setOffset": "{\n  this.offset=offset;\n}\n",
                "getOffset": "{\n  return offset;\n}\n",
                "getLength": "{\n  return length;\n}\n",
                "toString": "{\n  StringBuilder result=new StringBuilder();\n  result.append(offset);\n  result.append(',');\n  result.append(length);\n  for (  String h : hosts) {\n    result.append(',');\n    result.append(h);\n  }\n  return result.toString();\n}\n",
                "getNames": "{\n  if ((names == null) || (names.length == 0)) {\n    return new String[0];\n  }\n else {\n    return this.names;\n  }\n}\n",
                "BlockLocation": "{\n  if (names == null) {\n    this.names=new String[0];\n  }\n else {\n    this.names=names;\n  }\n  if (hosts == null) {\n    this.hosts=new String[0];\n  }\n else {\n    this.hosts=hosts;\n  }\n  this.offset=offset;\n  this.length=length;\n}\n",
                "write": "{\n  out.writeLong(offset);\n  out.writeLong(length);\n  out.writeInt(names.length);\n  for (int i=0; i < names.length; i++) {\n    Text name=new Text(names[i]);\n    name.write(out);\n  }\n  out.writeInt(hosts.length);\n  for (int i=0; i < hosts.length; i++) {\n    Text host=new Text(hosts[i]);\n    host.write(out);\n  }\n}\n",
                "getHosts": "{\n  if ((hosts == null) || (hosts.length == 0)) {\n    return new String[0];\n  }\n else {\n    return hosts;\n  }\n}\n"
            },
            "className": "org.apache.hadoop.fs.BlockLocation",
            "body": "class BlockLocation{private String[] hosts; private String[] names; private long offset; private long length; public BlockLocation(){} public BlockLocation(String[] names,String[] hosts,long offset,long length){} public String[] getHosts() throws IOException {} public String[] getNames() throws IOException {} public long getOffset(){} public long getLength(){} public void setOffset(long offset){} public void setLength(long length){} public void setHosts(String[] hosts) throws IOException {} public void setNames(String[] names) throws IOException {} public void write(DataOutput out) throws IOException {} public void readFields(DataInput in) throws IOException {} public String toString(){}  }",
            "fullmethod": {
                "setHosts": "/** \n * Set the hosts hosting this block\n */\npublic void setHosts(String[] hosts) throws IOException {\n  if (hosts == null) {\n    this.hosts=new String[0];\n  }\n else {\n    this.hosts=hosts;\n  }\n}\n",
                "readFields": "/** \n * Implement readFields of Writable\n */\npublic void readFields(DataInput in) throws IOException {\n  this.offset=in.readLong();\n  this.length=in.readLong();\n  int numNames=in.readInt();\n  this.names=new String[numNames];\n  for (int i=0; i < numNames; i++) {\n    Text name=new Text();\n    name.readFields(in);\n    names[i]=name.toString();\n  }\n  int numHosts=in.readInt();\n  for (int i=0; i < numHosts; i++) {\n    Text host=new Text();\n    host.readFields(in);\n    hosts[i]=host.toString();\n  }\n}\n",
                "setNames": "/** \n * Set the names (host:port) hosting this block\n */\npublic void setNames(String[] names) throws IOException {\n  if (names == null) {\n    this.names=new String[0];\n  }\n else {\n    this.names=names;\n  }\n}\n",
                "setLength": "/** \n * Set the length of block\n */\npublic void setLength(long length){\n  this.length=length;\n}\n",
                "setOffset": "/** \n * Set the start offset of file associated with this block\n */\npublic void setOffset(long offset){\n  this.offset=offset;\n}\n",
                "getOffset": "/** \n * Get the start offset of file associated with this block\n */\npublic long getOffset(){\n  return offset;\n}\n",
                "getLength": "/** \n * Get the length of the block\n */\npublic long getLength(){\n  return length;\n}\n",
                "toString": "public String toString(){\n  StringBuilder result=new StringBuilder();\n  result.append(offset);\n  result.append(',');\n  result.append(length);\n  for (  String h : hosts) {\n    result.append(',');\n    result.append(h);\n  }\n  return result.toString();\n}\n",
                "getNames": "/** \n * Get the list of names (hostname:port) hosting this block\n */\npublic String[] getNames() throws IOException {\n  if ((names == null) || (names.length == 0)) {\n    return new String[0];\n  }\n else {\n    return this.names;\n  }\n}\n",
                "BlockLocation": "/** \n * Constructor with host, name, offset and length\n */\npublic BlockLocation(String[] names,String[] hosts,long offset,long length){\n  if (names == null) {\n    this.names=new String[0];\n  }\n else {\n    this.names=names;\n  }\n  if (hosts == null) {\n    this.hosts=new String[0];\n  }\n else {\n    this.hosts=hosts;\n  }\n  this.offset=offset;\n  this.length=length;\n}\n",
                "write": "/** \n * Implement write of Writable\n */\npublic void write(DataOutput out) throws IOException {\n  out.writeLong(offset);\n  out.writeLong(length);\n  out.writeInt(names.length);\n  for (int i=0; i < names.length; i++) {\n    Text name=new Text(names[i]);\n    name.write(out);\n  }\n  out.writeInt(hosts.length);\n  for (int i=0; i < hosts.length; i++) {\n    Text host=new Text(hosts[i]);\n    host.write(out);\n  }\n}\n",
                "getHosts": "/** \n * Get the list of hosts (hostname) hosting this block\n */\npublic String[] getHosts() throws IOException {\n  if ((hosts == null) || (hosts.length == 0)) {\n    return new String[0];\n  }\n else {\n    return hosts;\n  }\n}\n"
            },
            "deps": [
                "org.apache.hadoop.fs.FilterFileSystem Return org.apache.hadoop.fs.BlockLocation",
                "org.apache.hadoop.mapred.MultiFileSplit Call org.apache.hadoop.fs.BlockLocation",
                "org.apache.hadoop.mapred.MultiFileSplit Import org.apache.hadoop.fs.BlockLocation",
                "org.apache.hadoop.mapred.MultiFileSplit Use org.apache.hadoop.fs.BlockLocation",
                "org.apache.hadoop.mapred.MultiFileSplit Contain org.apache.hadoop.fs.BlockLocation",
                "org.apache.hadoop.fs.kfs.KosmosFileSystem Call org.apache.hadoop.fs.BlockLocation",
                "org.apache.hadoop.fs.kfs.KosmosFileSystem Import org.apache.hadoop.fs.BlockLocation",
                "org.apache.hadoop.fs.kfs.KosmosFileSystem Return org.apache.hadoop.fs.BlockLocation",
                "org.apache.hadoop.fs.kfs.KosmosFileSystem Create org.apache.hadoop.fs.BlockLocation",
                "org.apache.hadoop.fs.kfs.KosmosFileSystem Use org.apache.hadoop.fs.BlockLocation",
                "org.apache.hadoop.fs.kfs.KosmosFileSystem Contain org.apache.hadoop.fs.BlockLocation",
                "org.apache.hadoop.fs.BlockLocation Return org.apache.hadoop.io.Writable",
                "org.apache.hadoop.fs.BlockLocation Implement org.apache.hadoop.io.Writable",
                "org.apache.hadoop.fs.BlockLocation Call org.apache.hadoop.io.WritableFactory",
                "org.apache.hadoop.fs.BlockLocation Create org.apache.hadoop.io.WritableFactory",
                "org.apache.hadoop.fs.BlockLocation Call org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.fs.BlockLocation Use org.apache.hadoop.io.WritableFactories"
            ],
            "directory": "org.apache.hadoop.fs"
        },
        "org.apache.hadoop.io.compress.CompressionOutputStream": {
            "method": {
                "CompressionOutputStream": "{\n  this.out=out;\n}\n",
                "flush": "{\n  out.flush();\n}\n",
                "close": "{\n  finish();\n  out.close();\n}\n"
            },
            "className": "org.apache.hadoop.io.compress.CompressionOutputStream",
            "body": "class CompressionOutputStream{protected final OutputStream out; protected CompressionOutputStream(OutputStream out){} public void close() throws IOException {} public void flush() throws IOException {} public abstract void write(byte[] b,int off,int len) throws IOException ;\n public abstract void finish() throws IOException ;\n public abstract void resetState() throws IOException ;\n  }",
            "classAnnotation": "/** \n * A compression output stream.\n */\n",
            "fullmethod": {
                "CompressionOutputStream": "/** \n * Create a compression output stream that writes the compressed bytes to the given stream.\n * @param out\n */\nprotected CompressionOutputStream(OutputStream out){\n  this.out=out;\n}\n",
                "flush": "public void flush() throws IOException {\n  out.flush();\n}\n",
                "close": "public void close() throws IOException {\n  finish();\n  out.close();\n}\n"
            },
            "deps": [
                "org.apache.hadoop.io.compress.CompressionCodec Return org.apache.hadoop.io.compress.CompressionOutputStream",
                "org.apache.hadoop.io.compress.BlockCompressorStream Use org.apache.hadoop.io.compress.CompressionOutputStream",
                "org.apache.hadoop.io.compress.DefaultCodec Return org.apache.hadoop.io.compress.CompressionOutputStream",
                "org.apache.hadoop.io.compress.CompressorStream Call org.apache.hadoop.io.compress.CompressionOutputStream",
                "org.apache.hadoop.io.compress.CompressorStream Import org.apache.hadoop.io.compress.CompressionOutputStream",
                "org.apache.hadoop.io.compress.CompressorStream Extend org.apache.hadoop.io.compress.CompressionOutputStream",
                "org.apache.hadoop.io.compress.CompressorStream Use org.apache.hadoop.io.compress.CompressionOutputStream",
                "org.apache.hadoop.io.compress.CompressionCodecFactory Call org.apache.hadoop.io.compress.CompressionOutputStream",
                "org.apache.hadoop.io.compress.CompressionCodecFactory Contain org.apache.hadoop.io.compress.CompressionOutputStream",
                "org.apache.hadoop.io.compress.LzoCodec Return org.apache.hadoop.io.compress.CompressionOutputStream"
            ],
            "directory": "org.apache.hadoop.io.compress"
        },
        "org.apache.hadoop.hdfs.server.namenode.FileDataServlet": {
            "method": {
                "pickSrcDatanode": "{\n  final LocatedBlocks blks=nnproxy.getBlockLocations(i.getPath().toUri().getPath(),0,1);\n  if (i.getLen() == 0 || blks.getLocatedBlocks().size() <= 0) {\n    return jspHelper.randomNode();\n  }\n  return jspHelper.bestNode(blks.get(0));\n}\n",
                "createUri": "{\n  final DatanodeID host=pickSrcDatanode(i,nnproxy);\n  final String hostname;\n  if (host instanceof DatanodeInfo) {\n    hostname=((DatanodeInfo)host).getHostName();\n  }\n else {\n    hostname=host.getHost();\n  }\n  return new URI(scheme,null,hostname,\"https\".equals(scheme) ? (Integer)getServletContext().getAttribute(\"datanode.https.port\") : host.getInfoPort(),\"/streamFile\",\"filename=\" + i.getPath() + \"&ugi=\"+ ugi,null);\n}\n",
                "doGet": "{\n  final UnixUserGroupInformation ugi=getUGI(request);\n  final ClientProtocol nnproxy=createNameNodeProxy(ugi);\n  try {\n    final String path=request.getPathInfo() != null ? request.getPathInfo() : \"/\";\n    FileStatus info=nnproxy.getFileInfo(path);\n    if ((info != null) && !info.isDir()) {\n      response.sendRedirect(createUri(info,ugi,nnproxy,request.getScheme()).toURL().toString());\n    }\n else     if (info == null) {\n      response.sendError(400,\"cat: File not found \" + path);\n    }\n else {\n      response.sendError(400,\"cat: \" + path + \": is a directory\");\n    }\n  }\n catch (  URISyntaxException e) {\n    response.getWriter().println(e.toString());\n  }\ncatch (  IOException e) {\n    response.sendError(400,e.getMessage());\n  }\n}\n"
            },
            "className": "org.apache.hadoop.hdfs.server.namenode.FileDataServlet",
            "body": "class FileDataServlet{private final static JspHelper jspHelper=new JspHelper(); private URI createUri(FileStatus i,UnixUserGroupInformation ugi,ClientProtocol nnproxy,String scheme) throws IOException, URISyntaxException {} private static DatanodeID pickSrcDatanode(FileStatus i,ClientProtocol nnproxy) throws IOException {} public void doGet(HttpServletRequest request,HttpServletResponse response) throws IOException {}  }",
            "classAnnotation": "/** \n * Redirect queries about the hosted filesystem to an appropriate datanode.\n * @see org.apache.hadoop.hdfs.HftpFileSystem\n */\n",
            "fullmethod": {
                "pickSrcDatanode": "/** \n * Select a datanode to service this request. Currently, this looks at no more than the first five blocks of a file, selecting a datanode randomly from the most represented.\n */\nprivate static DatanodeID pickSrcDatanode(FileStatus i,ClientProtocol nnproxy) throws IOException {\n  final LocatedBlocks blks=nnproxy.getBlockLocations(i.getPath().toUri().getPath(),0,1);\n  if (i.getLen() == 0 || blks.getLocatedBlocks().size() <= 0) {\n    return jspHelper.randomNode();\n  }\n  return jspHelper.bestNode(blks.get(0));\n}\n",
                "createUri": "private URI createUri(FileStatus i,UnixUserGroupInformation ugi,ClientProtocol nnproxy,String scheme) throws IOException, URISyntaxException {\n  final DatanodeID host=pickSrcDatanode(i,nnproxy);\n  final String hostname;\n  if (host instanceof DatanodeInfo) {\n    hostname=((DatanodeInfo)host).getHostName();\n  }\n else {\n    hostname=host.getHost();\n  }\n  return new URI(scheme,null,hostname,\"https\".equals(scheme) ? (Integer)getServletContext().getAttribute(\"datanode.https.port\") : host.getInfoPort(),\"/streamFile\",\"filename=\" + i.getPath() + \"&ugi=\"+ ugi,null);\n}\n",
                "doGet": "/** \n * Service a GET request as described below. Request: {@code GET http://<nn>:<port>/data[/<path>] HTTP/1.1}\n */\npublic void doGet(HttpServletRequest request,HttpServletResponse response) throws IOException {\n  final UnixUserGroupInformation ugi=getUGI(request);\n  final ClientProtocol nnproxy=createNameNodeProxy(ugi);\n  try {\n    final String path=request.getPathInfo() != null ? request.getPathInfo() : \"/\";\n    FileStatus info=nnproxy.getFileInfo(path);\n    if ((info != null) && !info.isDir()) {\n      response.sendRedirect(createUri(info,ugi,nnproxy,request.getScheme()).toURL().toString());\n    }\n else     if (info == null) {\n      response.sendError(400,\"cat: File not found \" + path);\n    }\n else {\n      response.sendError(400,\"cat: \" + path + \": is a directory\");\n    }\n  }\n catch (  URISyntaxException e) {\n    response.getWriter().println(e.toString());\n  }\ncatch (  IOException e) {\n    response.sendError(400,e.getMessage());\n  }\n}\n"
            },
            "deps": [
                "org.apache.hadoop.hdfs.server.namenode.FileDataServlet Call org.apache.hadoop.fs.FileStatus",
                "org.apache.hadoop.hdfs.server.namenode.FileDataServlet Import org.apache.hadoop.fs.FileStatus",
                "org.apache.hadoop.hdfs.server.namenode.FileDataServlet Parameter org.apache.hadoop.fs.FileStatus",
                "org.apache.hadoop.hdfs.server.namenode.FileDataServlet Contain org.apache.hadoop.fs.FileStatus",
                "org.apache.hadoop.hdfs.server.namenode.FileDataServlet Call org.apache.hadoop.fs.Path",
                "org.apache.hadoop.hdfs.server.namenode.FileDataServlet Call org.apache.hadoop.hdfs.protocol.DatanodeID",
                "org.apache.hadoop.hdfs.server.namenode.FileDataServlet Import org.apache.hadoop.hdfs.protocol.DatanodeID",
                "org.apache.hadoop.hdfs.server.namenode.FileDataServlet Return org.apache.hadoop.hdfs.protocol.DatanodeID",
                "org.apache.hadoop.hdfs.server.namenode.FileDataServlet Contain org.apache.hadoop.hdfs.protocol.DatanodeID",
                "org.apache.hadoop.hdfs.server.namenode.FileDataServlet Call org.apache.hadoop.hdfs.protocol.ClientProtocol",
                "org.apache.hadoop.hdfs.server.namenode.FileDataServlet Import org.apache.hadoop.hdfs.protocol.ClientProtocol",
                "org.apache.hadoop.hdfs.server.namenode.FileDataServlet Parameter org.apache.hadoop.hdfs.protocol.ClientProtocol",
                "org.apache.hadoop.hdfs.server.namenode.FileDataServlet Contain org.apache.hadoop.hdfs.protocol.ClientProtocol",
                "org.apache.hadoop.hdfs.server.namenode.FileDataServlet Cast org.apache.hadoop.hdfs.protocol.DatanodeInfo",
                "org.apache.hadoop.hdfs.server.namenode.FileDataServlet Call org.apache.hadoop.hdfs.protocol.DatanodeInfo",
                "org.apache.hadoop.hdfs.server.namenode.FileDataServlet Import org.apache.hadoop.hdfs.protocol.DatanodeInfo",
                "org.apache.hadoop.hdfs.server.namenode.FileDataServlet Use org.apache.hadoop.hdfs.protocol.DatanodeInfo",
                "org.apache.hadoop.hdfs.server.namenode.FileDataServlet Call org.apache.hadoop.hdfs.protocol.LocatedBlocks",
                "org.apache.hadoop.hdfs.server.namenode.FileDataServlet Import org.apache.hadoop.hdfs.protocol.LocatedBlocks",
                "org.apache.hadoop.hdfs.server.namenode.FileDataServlet Contain org.apache.hadoop.hdfs.protocol.LocatedBlocks",
                "org.apache.hadoop.hdfs.server.namenode.FileDataServlet Import org.apache.hadoop.security.UnixUserGroupInformation",
                "org.apache.hadoop.hdfs.server.namenode.FileDataServlet Parameter org.apache.hadoop.security.UnixUserGroupInformation",
                "org.apache.hadoop.hdfs.server.namenode.FileDataServlet Contain org.apache.hadoop.security.UnixUserGroupInformation",
                "org.apache.hadoop.hdfs.server.namenode.FileDataServlet Call org.apache.hadoop.hdfs.server.namenode.DfsServlet",
                "org.apache.hadoop.hdfs.server.namenode.FileDataServlet Extend org.apache.hadoop.hdfs.server.namenode.DfsServlet"
            ],
            "directory": "org.apache.hadoop.hdfs.server.namenode"
        },
        "org.apache.hadoop.record.RecordComparator": {
            "method": {
                "define": "{\n  WritableComparator.define(c,comparator);\n}\n",
                "RecordComparator": "{\n  super(recordClass);\n}\n"
            },
            "className": "org.apache.hadoop.record.RecordComparator",
            "body": "class RecordComparator{protected RecordComparator(Class<? extends WritableComparable> recordClass){} public abstract int compare(byte[] b1,int s1,int l1,byte[] b2,int s2,int l2);\n public static synchronized void define(Class c,RecordComparator comparator){}  }",
            "classAnnotation": "/** \n * A raw record comparator base class\n */\n",
            "fullmethod": {
                "define": "/** \n * Register an optimized comparator for a  {@link Record} implementation.\n * @param c record classs for which a raw comparator is provided\n * @param comparator Raw comparator instance for class c \n */\npublic static synchronized void define(Class c,RecordComparator comparator){\n  WritableComparator.define(c,comparator);\n}\n",
                "RecordComparator": "/** \n * Construct a raw  {@link Record} comparison implementation. \n */\nprotected RecordComparator(Class<? extends WritableComparable> recordClass){\n  super(recordClass);\n}\n"
            },
            "deps": [
                "org.apache.hadoop.record.RecordComparator Import org.apache.hadoop.io.WritableComparable",
                "org.apache.hadoop.record.RecordComparator Call org.apache.hadoop.io.WritableComparator",
                "org.apache.hadoop.record.RecordComparator Import org.apache.hadoop.io.WritableComparator",
                "org.apache.hadoop.record.RecordComparator Extend org.apache.hadoop.io.WritableComparator",
                "org.apache.hadoop.record.RecordComparator Use org.apache.hadoop.io.WritableComparator"
            ],
            "directory": "org.apache.hadoop.record"
        },
        "org.apache.hadoop.mapred.lib.aggregate.UserDefinedValueAggregatorDescriptor": {
            "method": {
                "createInstance": "{\n  Object retv=null;\n  try {\n    ClassLoader classLoader=Thread.currentThread().getContextClassLoader();\n    Class<?> theFilterClass=Class.forName(className,true,classLoader);\n    Constructor meth=theFilterClass.getDeclaredConstructor(argArray);\n    meth.setAccessible(true);\n    retv=meth.newInstance();\n  }\n catch (  Exception e) {\n    throw new RuntimeException(e);\n  }\n  return retv;\n}\n",
                "UserDefinedValueAggregatorDescriptor": "{\n  this.className=className;\n  this.createAggregator(job);\n}\n",
                "toString": "{\n  return \"UserDefinedValueAggregatorDescriptor with class name:\" + \"\\t\" + this.className;\n}\n",
                "createAggregator": "{\n  if (theAggregatorDescriptor == null) {\n    theAggregatorDescriptor=(ValueAggregatorDescriptor)createInstance(this.className);\n    theAggregatorDescriptor.configure(job);\n  }\n}\n",
                "configure": "{\n}\n",
                "generateKeyValPairs": "{\n  ArrayList<Entry<Text,Text>> retv=new ArrayList<Entry<Text,Text>>();\n  if (this.theAggregatorDescriptor != null) {\n    retv=this.theAggregatorDescriptor.generateKeyValPairs(key,val);\n  }\n  return retv;\n}\n"
            },
            "className": "org.apache.hadoop.mapred.lib.aggregate.UserDefinedValueAggregatorDescriptor",
            "body": "class UserDefinedValueAggregatorDescriptor{private String className; private ValueAggregatorDescriptor theAggregatorDescriptor=null; private static final Class[] argArray=new Class[]{}; public static Object createInstance(String className){} private void createAggregator(JobConf job){} public UserDefinedValueAggregatorDescriptor(String className,JobConf job){} public ArrayList<Entry<Text,Text>> generateKeyValPairs(Object key,Object val){} public String toString(){} public void configure(JobConf job){}  }",
            "classAnnotation": "/** \n * This class implements a wrapper for a user defined value aggregator descriptor. It servs two functions: One is to create an object of ValueAggregatorDescriptor from the name of a user defined class that may be dynamically loaded. The other is to deligate inviokations of generateKeyValPairs function to the created object.\n */\n",
            "fullmethod": {
                "createInstance": "/** \n * Create an instance of the given class\n * @param className the name of the class\n * @return a dynamically created instance of the given class \n */\npublic static Object createInstance(String className){\n  Object retv=null;\n  try {\n    ClassLoader classLoader=Thread.currentThread().getContextClassLoader();\n    Class<?> theFilterClass=Class.forName(className,true,classLoader);\n    Constructor meth=theFilterClass.getDeclaredConstructor(argArray);\n    meth.setAccessible(true);\n    retv=meth.newInstance();\n  }\n catch (  Exception e) {\n    throw new RuntimeException(e);\n  }\n  return retv;\n}\n",
                "UserDefinedValueAggregatorDescriptor": "/** \n * @param className the class name of the user defined descriptor class\n * @param job a configure object used for decriptor configuration\n */\npublic UserDefinedValueAggregatorDescriptor(String className,JobConf job){\n  this.className=className;\n  this.createAggregator(job);\n}\n",
                "toString": "/** \n * @return the string representation of this object.\n */\npublic String toString(){\n  return \"UserDefinedValueAggregatorDescriptor with class name:\" + \"\\t\" + this.className;\n}\n",
                "createAggregator": "private void createAggregator(JobConf job){\n  if (theAggregatorDescriptor == null) {\n    theAggregatorDescriptor=(ValueAggregatorDescriptor)createInstance(this.className);\n    theAggregatorDescriptor.configure(job);\n  }\n}\n",
                "configure": "/** \n * Do nothing.\n */\npublic void configure(JobConf job){\n}\n",
                "generateKeyValPairs": "/** \n * Generate a list of aggregation-id/value pairs for the given key/value pairs by delegating the invocation to the real object.\n * @param key input key\n * @param val input value\n * @return a list of aggregation id/value pairs. An aggregation id encodes anaggregation type which is used to guide the way to aggregate the value in the reduce/combiner phrase of an Aggregate based job.\n */\npublic ArrayList<Entry<Text,Text>> generateKeyValPairs(Object key,Object val){\n  ArrayList<Entry<Text,Text>> retv=new ArrayList<Entry<Text,Text>>();\n  if (this.theAggregatorDescriptor != null) {\n    retv=this.theAggregatorDescriptor.generateKeyValPairs(key,val);\n  }\n  return retv;\n}\n"
            },
            "deps": [
                "org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorJobBase Call org.apache.hadoop.mapred.lib.aggregate.UserDefinedValueAggregatorDescriptor",
                "org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorJobBase Create org.apache.hadoop.mapred.lib.aggregate.UserDefinedValueAggregatorDescriptor",
                "org.apache.hadoop.mapred.lib.aggregate.UserDefinedValueAggregatorDescriptor Cast org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorDescriptor",
                "org.apache.hadoop.mapred.lib.aggregate.UserDefinedValueAggregatorDescriptor Call org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorDescriptor",
                "org.apache.hadoop.mapred.lib.aggregate.UserDefinedValueAggregatorDescriptor Use org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorDescriptor",
                "org.apache.hadoop.mapred.lib.aggregate.UserDefinedValueAggregatorDescriptor Implement org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorDescriptor",
                "org.apache.hadoop.mapred.lib.aggregate.UserDefinedValueAggregatorDescriptor Contain org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorDescriptor",
                "org.apache.hadoop.mapred.lib.aggregate.UserDefinedValueAggregatorDescriptor Import org.apache.hadoop.mapred.JobConf",
                "org.apache.hadoop.mapred.lib.aggregate.UserDefinedValueAggregatorDescriptor Parameter org.apache.hadoop.mapred.JobConf"
            ],
            "directory": "org.apache.hadoop.mapred.lib.aggregate"
        },
        "org.apache.hadoop.mapred.InterTrackerProtocol": {
            "method": {},
            "className": "org.apache.hadoop.mapred.InterTrackerProtocol",
            "body": "class InterTrackerProtocol{public static final long versionID=20L; public final static int TRACKERS_OK=0; public final static int UNKNOWN_TASKTRACKER=1; HeartbeatResponse heartbeat(TaskTrackerStatus status,boolean initialContact,boolean acceptNewTasks,short responseId) throws IOException ;\n public String getFilesystemName() throws IOException ;\n public void reportTaskTrackerError(String taskTracker,String errorClass,String errorMessage) throws IOException ;\n TaskCompletionEvent[] getTaskCompletionEvents(JobID jobid,int fromEventId,int maxEvents) throws IOException ;\n public String getSystemDir();\n public String getBuildVersion() throws IOException ;\n  }",
            "classAnnotation": "/** \n * Protocol that a TaskTracker and the central JobTracker use to communicate. The JobTracker is the Server, which implements this protocol.\n */\n",
            "fullmethod": {},
            "deps": [
                "org.apache.hadoop.mapred.InterTrackerProtocol Parameter org.apache.hadoop.mapred.JobID",
                "org.apache.hadoop.mapred.InterTrackerProtocol Return org.apache.hadoop.mapred.HeartbeatResponse",
                "org.apache.hadoop.mapred.InterTrackerProtocol Return org.apache.hadoop.mapred.TaskCompletionEvent",
                "org.apache.hadoop.mapred.InterTrackerProtocol Import org.apache.hadoop.ipc.VersionedProtocol",
                "org.apache.hadoop.mapred.InterTrackerProtocol Extend org.apache.hadoop.ipc.VersionedProtocol"
            ],
            "directory": "org.apache.hadoop.mapred"
        },
        "org.apache.hadoop.hdfs.server.namenode.CheckpointSignature": {
            "method": {
                "readFields": "{\n  layoutVersion=in.readInt();\n  namespaceID=in.readInt();\n  cTime=in.readLong();\n  editsTime=in.readLong();\n  checkpointTime=in.readLong();\n}\n",
                "validateStorageInfo": "{\n  if (layoutVersion != si.layoutVersion || namespaceID != si.namespaceID || cTime != si.cTime) {\n    throw new IOException(\"Inconsistent checkpoint fileds. \" + \"LV = \" + layoutVersion + \" namespaceID = \"+ namespaceID+ \" cTime = \"+ cTime+ \". Expecting respectively: \"+ si.layoutVersion+ \"; \"+ si.namespaceID+ \"; \"+ si.cTime);\n  }\n}\n",
                "hashCode": "{\n  return layoutVersion ^ namespaceID ^ (int)(cTime ^ editsTime ^ checkpointTime);\n}\n",
                "equals": "{\n  if (!(o instanceof CheckpointSignature)) {\n    return false;\n  }\n  return compareTo((CheckpointSignature)o) == 0;\n}\n",
                "toString": "{\n  return String.valueOf(layoutVersion) + FIELD_SEPARATOR + String.valueOf(namespaceID)+ FIELD_SEPARATOR+ String.valueOf(cTime)+ FIELD_SEPARATOR+ String.valueOf(editsTime)+ FIELD_SEPARATOR+ String.valueOf(checkpointTime);\n}\n",
                "compareTo": "{\n  return (layoutVersion < o.layoutVersion) ? -1 : (layoutVersion > o.layoutVersion) ? 1 : (namespaceID < o.namespaceID) ? -1 : (namespaceID > o.namespaceID) ? 1 : (cTime < o.cTime) ? -1 : (cTime > o.cTime) ? 1 : (editsTime < o.editsTime) ? -1 : (editsTime > o.editsTime) ? 1 : (checkpointTime < o.checkpointTime) ? -1 : (checkpointTime > o.checkpointTime) ? 1 : 0;\n}\n",
                "write": "{\n  out.writeInt(getLayoutVersion());\n  out.writeInt(getNamespaceID());\n  out.writeLong(getCTime());\n  out.writeLong(editsTime);\n  out.writeLong(checkpointTime);\n}\n",
                "CheckpointSignature": "{\n  String[] fields=str.split(FIELD_SEPARATOR);\n  assert fields.length == 5 : \"Must be 5 fields in CheckpointSignature\";\n  layoutVersion=Integer.valueOf(fields[0]);\n  namespaceID=Integer.valueOf(fields[1]);\n  cTime=Long.valueOf(fields[2]);\n  editsTime=Long.valueOf(fields[3]);\n  checkpointTime=Long.valueOf(fields[4]);\n}\n"
            },
            "className": "org.apache.hadoop.hdfs.server.namenode.CheckpointSignature",
            "body": "class CheckpointSignature{private static final String FIELD_SEPARATOR=\":\"; long editsTime=-1L; long checkpointTime=-1L; CheckpointSignature(){} CheckpointSignature(FSImage fsImage){} CheckpointSignature(String str){} public String toString(){} void validateStorageInfo(StorageInfo si) throws IOException {} public int compareTo(CheckpointSignature o){} public boolean equals(Object o){} public int hashCode(){} public void write(DataOutput out) throws IOException {} public void readFields(DataInput in) throws IOException {}  }",
            "classAnnotation": "/** \n * A unique signature intended to identify checkpoint transactions.\n */\n",
            "fullmethod": {
                "readFields": "public void readFields(DataInput in) throws IOException {\n  layoutVersion=in.readInt();\n  namespaceID=in.readInt();\n  cTime=in.readLong();\n  editsTime=in.readLong();\n  checkpointTime=in.readLong();\n}\n",
                "validateStorageInfo": "void validateStorageInfo(StorageInfo si) throws IOException {\n  if (layoutVersion != si.layoutVersion || namespaceID != si.namespaceID || cTime != si.cTime) {\n    throw new IOException(\"Inconsistent checkpoint fileds. \" + \"LV = \" + layoutVersion + \" namespaceID = \"+ namespaceID+ \" cTime = \"+ cTime+ \". Expecting respectively: \"+ si.layoutVersion+ \"; \"+ si.namespaceID+ \"; \"+ si.cTime);\n  }\n}\n",
                "hashCode": "public int hashCode(){\n  return layoutVersion ^ namespaceID ^ (int)(cTime ^ editsTime ^ checkpointTime);\n}\n",
                "equals": "public boolean equals(Object o){\n  if (!(o instanceof CheckpointSignature)) {\n    return false;\n  }\n  return compareTo((CheckpointSignature)o) == 0;\n}\n",
                "toString": "public String toString(){\n  return String.valueOf(layoutVersion) + FIELD_SEPARATOR + String.valueOf(namespaceID)+ FIELD_SEPARATOR+ String.valueOf(cTime)+ FIELD_SEPARATOR+ String.valueOf(editsTime)+ FIELD_SEPARATOR+ String.valueOf(checkpointTime);\n}\n",
                "compareTo": "public int compareTo(CheckpointSignature o){\n  return (layoutVersion < o.layoutVersion) ? -1 : (layoutVersion > o.layoutVersion) ? 1 : (namespaceID < o.namespaceID) ? -1 : (namespaceID > o.namespaceID) ? 1 : (cTime < o.cTime) ? -1 : (cTime > o.cTime) ? 1 : (editsTime < o.editsTime) ? -1 : (editsTime > o.editsTime) ? 1 : (checkpointTime < o.checkpointTime) ? -1 : (checkpointTime > o.checkpointTime) ? 1 : 0;\n}\n",
                "write": "public void write(DataOutput out) throws IOException {\n  out.writeInt(getLayoutVersion());\n  out.writeInt(getNamespaceID());\n  out.writeLong(getCTime());\n  out.writeLong(editsTime);\n  out.writeLong(checkpointTime);\n}\n",
                "CheckpointSignature": "CheckpointSignature(String str){\n  String[] fields=str.split(FIELD_SEPARATOR);\n  assert fields.length == 5 : \"Must be 5 fields in CheckpointSignature\";\n  layoutVersion=Integer.valueOf(fields[0]);\n  namespaceID=Integer.valueOf(fields[1]);\n  cTime=Long.valueOf(fields[2]);\n  editsTime=Long.valueOf(fields[3]);\n  checkpointTime=Long.valueOf(fields[4]);\n}\n"
            },
            "deps": [
                "org.apache.hadoop.hdfs.server.namenode.TransferFsImage Call org.apache.hadoop.hdfs.server.namenode.CheckpointSignature",
                "org.apache.hadoop.hdfs.server.namenode.TransferFsImage Return org.apache.hadoop.hdfs.server.namenode.CheckpointSignature",
                "org.apache.hadoop.hdfs.server.namenode.TransferFsImage Use org.apache.hadoop.hdfs.server.namenode.CheckpointSignature",
                "org.apache.hadoop.hdfs.server.namenode.TransferFsImage Create org.apache.hadoop.hdfs.server.namenode.CheckpointSignature",
                "org.apache.hadoop.hdfs.server.namenode.TransferFsImage Contain org.apache.hadoop.hdfs.server.namenode.CheckpointSignature",
                "org.apache.hadoop.hdfs.server.namenode.CheckpointSignature Call org.apache.hadoop.hdfs.server.common.StorageInfo",
                "org.apache.hadoop.hdfs.server.namenode.CheckpointSignature Import org.apache.hadoop.hdfs.server.common.StorageInfo",
                "org.apache.hadoop.hdfs.server.namenode.CheckpointSignature Extend org.apache.hadoop.hdfs.server.common.StorageInfo",
                "org.apache.hadoop.hdfs.server.namenode.CheckpointSignature Use org.apache.hadoop.hdfs.server.common.StorageInfo",
                "org.apache.hadoop.hdfs.server.protocol.NamenodeProtocol Import org.apache.hadoop.hdfs.server.namenode.CheckpointSignature",
                "org.apache.hadoop.hdfs.server.protocol.NamenodeProtocol Return org.apache.hadoop.hdfs.server.namenode.CheckpointSignature",
                "org.apache.hadoop.hdfs.server.namenode.CheckpointSignature Import org.apache.hadoop.io.WritableComparable",
                "org.apache.hadoop.hdfs.server.namenode.CheckpointSignature Implement org.apache.hadoop.io.WritableComparable",
                "org.apache.hadoop.hdfs.server.namenode.NameNode Return org.apache.hadoop.hdfs.server.namenode.CheckpointSignature"
            ],
            "directory": "org.apache.hadoop.hdfs.server.namenode"
        },
        "org.apache.hadoop.io.WritableFactories": {
            "method": {
                "setFactory": "{\n  CLASS_TO_FACTORY.put(c,factory);\n}\n",
                "WritableFactories": "{\n}\n",
                "newInstance": "{\n  return newInstance(c,null);\n}\n",
                "getFactory": "{\n  return CLASS_TO_FACTORY.get(c);\n}\n"
            },
            "className": "org.apache.hadoop.io.WritableFactories",
            "body": "class WritableFactories{private static final HashMap<Class,WritableFactory> CLASS_TO_FACTORY=new HashMap<Class,WritableFactory>(); private WritableFactories(){} public static synchronized void setFactory(Class c,WritableFactory factory){} public static synchronized WritableFactory getFactory(Class c){} public static Writable newInstance(Class<? extends Writable> c,Configuration conf){} public static Writable newInstance(Class<? extends Writable> c){}  }",
            "classAnnotation": "/** \n * Factories for non-public writables.  Defining a factory permits  {@link ObjectWritable} to be able to construct instances of non-public classes. \n */\n",
            "fullmethod": {
                "setFactory": "/** \n * Define a factory for a class. \n */\npublic static synchronized void setFactory(Class c,WritableFactory factory){\n  CLASS_TO_FACTORY.put(c,factory);\n}\n",
                "WritableFactories": "private WritableFactories(){\n}\n",
                "newInstance": "/** \n * Create a new instance of a class with a defined factory. \n */\npublic static Writable newInstance(Class<? extends Writable> c){\n  return newInstance(c,null);\n}\n",
                "getFactory": "/** \n * Define a factory for a class. \n */\npublic static synchronized WritableFactory getFactory(Class c){\n  return CLASS_TO_FACTORY.get(c);\n}\n"
            },
            "deps": [
                "org.apache.hadoop.hdfs.protocol.DatanodeInfo Call org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.hdfs.protocol.DatanodeInfo Import org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.hdfs.protocol.DatanodeInfo Use org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.hdfs.server.protocol.UpgradeCommand Call org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.hdfs.server.protocol.UpgradeCommand Import org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.hdfs.server.protocol.UpgradeCommand Use org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.io.WritableFactories Call org.apache.hadoop.io.WritableFactory",
                "org.apache.hadoop.io.WritableFactories Return org.apache.hadoop.io.WritableFactory",
                "org.apache.hadoop.io.WritableFactories Use org.apache.hadoop.io.WritableFactory",
                "org.apache.hadoop.io.WritableFactories Parameter org.apache.hadoop.io.WritableFactory",
                "org.apache.hadoop.io.WritableFactories Contain org.apache.hadoop.io.WritableFactory",
                "org.apache.hadoop.hdfs.server.common.GenerationStamp Call org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.hdfs.server.common.GenerationStamp Use org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.io.WritableFactories Return org.apache.hadoop.io.Writable",
                "org.apache.hadoop.io.WritableFactories Contain org.apache.hadoop.io.Writable",
                "org.apache.hadoop.hdfs.server.protocol.BlockCommand Call org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.hdfs.server.protocol.BlockCommand Use org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.io.WritableFactories Cast org.apache.hadoop.conf.Configurable",
                "org.apache.hadoop.io.WritableFactories Call org.apache.hadoop.conf.Configurable",
                "org.apache.hadoop.io.WritableFactories Use org.apache.hadoop.conf.Configurable",
                "org.apache.hadoop.fs.permission.PermissionStatus Call org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.fs.permission.PermissionStatus Use org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.mapred.JobStatus Call org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.mapred.JobStatus Import org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.mapred.JobStatus Use org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.mapred.JobProfile Call org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.mapred.JobProfile Import org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.mapred.JobProfile Use org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.fs.BlockLocation Call org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.fs.BlockLocation Use org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.hdfs.protocol.LocatedBlocks Call org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.hdfs.protocol.LocatedBlocks Import org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.hdfs.protocol.LocatedBlocks Use org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.fs.permission.FsPermission Call org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.fs.permission.FsPermission Use org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.io.ArrayWritable Call org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.io.ArrayWritable Use org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.hdfs.protocol.LocatedBlock Call org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.hdfs.protocol.LocatedBlock Use org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.io.WritableFactories Import org.apache.hadoop.util.ReflectionUtils",
                "org.apache.hadoop.io.WritableFactories Use org.apache.hadoop.util.ReflectionUtils",
                "org.apache.hadoop.hdfs.protocol.Block Call org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.hdfs.protocol.Block Use org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.hdfs.server.protocol.NamespaceInfo Call org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.hdfs.server.protocol.NamespaceInfo Import org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.hdfs.server.protocol.NamespaceInfo Use org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.hdfs.server.protocol.BlockMetaDataInfo Call org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.hdfs.server.protocol.BlockMetaDataInfo Use org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.hdfs.server.common.UpgradeStatusReport Call org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.hdfs.server.common.UpgradeStatusReport Import org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.hdfs.server.common.UpgradeStatusReport Use org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration Call org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration Import org.apache.hadoop.io.WritableFactories",
                "org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration Use org.apache.hadoop.io.WritableFactories"
            ],
            "directory": "org.apache.hadoop.io"
        },
        "org.apache.hadoop.mapred.TaskRunner": {
            "method": {
                "prepare": "{\n  return true;\n}\n",
                "signalDone": "{\nsynchronized (lock) {\n    done=true;\n    lock.notify();\n  }\n}\n",
                "getTaskInProgress": "{\n  return tip;\n}\n",
                "setupWorkDir": "{\n  File workDir=new File(\".\").getAbsoluteFile();\n  FileUtil.fullyDelete(workDir);\n  if (DistributedCache.getSymlink(conf)) {\n    URI[] archives=DistributedCache.getCacheArchives(conf);\n    URI[] files=DistributedCache.getCacheFiles(conf);\n    Path[] localArchives=DistributedCache.getLocalCacheArchives(conf);\n    Path[] localFiles=DistributedCache.getLocalCacheFiles(conf);\n    if (archives != null) {\n      for (int i=0; i < archives.length; i++) {\n        String link=archives[i].getFragment();\n        if (link != null) {\n          link=workDir.toString() + Path.SEPARATOR + link;\n          File flink=new File(link);\n          if (!flink.exists()) {\n            FileUtil.symLink(localArchives[i].toString(),link);\n          }\n        }\n      }\n    }\n    if (files != null) {\n      for (int i=0; i < files.length; i++) {\n        String link=files[i].getFragment();\n        if (link != null) {\n          link=workDir.toString() + Path.SEPARATOR + link;\n          File flink=new File(link);\n          if (!flink.exists()) {\n            FileUtil.symLink(localFiles[i].toString(),link);\n          }\n        }\n      }\n    }\n  }\n  File jobCacheDir=null;\n  if (conf.getJar() != null) {\n    jobCacheDir=new File(new Path(conf.getJar()).getParent().toString());\n  }\n  try {\n    DistributedCache.createAllSymlink(conf,jobCacheDir,workDir);\n  }\n catch (  IOException ie) {\n    LOG.warn(StringUtils.stringifyException(ie));\n  }\n  String tmp=conf.get(\"mapred.child.tmp\",\"./tmp\");\n  Path tmpDir=new Path(tmp);\n  if (!tmpDir.isAbsolute()) {\n    tmpDir=new Path(workDir.toString(),tmp);\n    FileSystem localFs=FileSystem.getLocal(conf);\n    if (!localFs.mkdirs(tmpDir) && !localFs.getFileStatus(tmpDir).isDir()) {\n      throw new IOException(\"Mkdirs failed to create \" + tmpDir.toString());\n    }\n  }\n}\n",
                "setExitCode": "{\n  this.exitCodeSet=true;\n  this.exitCode=exitCode;\n}\n",
                "run": "{\n  try {\n    TaskAttemptID taskid=t.getTaskID();\n    LocalDirAllocator lDirAlloc=new LocalDirAllocator(\"mapred.local.dir\");\n    File jobCacheDir=null;\n    if (conf.getJar() != null) {\n      jobCacheDir=new File(new Path(conf.getJar()).getParent().toString());\n    }\n    File workDir=new File(lDirAlloc.getLocalPathToRead(TaskTracker.getJobCacheSubdir() + Path.SEPARATOR + t.getJobID()+ Path.SEPARATOR+ t.getTaskID()+ Path.SEPARATOR+ MRConstants.WORKDIR,conf).toString());\n    URI[] archives=DistributedCache.getCacheArchives(conf);\n    URI[] files=DistributedCache.getCacheFiles(conf);\n    FileStatus fileStatus;\n    FileSystem fileSystem;\n    Path localPath;\n    String baseDir;\n    if ((archives != null) || (files != null)) {\n      if (archives != null) {\n        String[] archivesTimestamps=DistributedCache.getArchiveTimestamps(conf);\n        Path[] p=new Path[archives.length];\n        for (int i=0; i < archives.length; i++) {\n          fileSystem=FileSystem.get(archives[i],conf);\n          fileStatus=fileSystem.getFileStatus(new Path(archives[i].getPath()));\n          String cacheId=DistributedCache.makeRelative(archives[i],conf);\n          String cachePath=TaskTracker.getCacheSubdir() + Path.SEPARATOR + cacheId;\n          if (lDirAlloc.ifExists(cachePath,conf)) {\n            localPath=lDirAlloc.getLocalPathToRead(cachePath,conf);\n          }\n else {\n            localPath=lDirAlloc.getLocalPathForWrite(cachePath,fileStatus.getLen(),conf);\n          }\n          baseDir=localPath.toString().replace(cacheId,\"\");\n          p[i]=DistributedCache.getLocalCache(archives[i],conf,new Path(baseDir),fileStatus,true,Long.parseLong(archivesTimestamps[i]),new Path(workDir.getAbsolutePath()),false);\n        }\n        DistributedCache.setLocalArchives(conf,stringifyPathArray(p));\n      }\n      if ((files != null)) {\n        String[] fileTimestamps=DistributedCache.getFileTimestamps(conf);\n        Path[] p=new Path[files.length];\n        for (int i=0; i < files.length; i++) {\n          fileSystem=FileSystem.get(files[i],conf);\n          fileStatus=fileSystem.getFileStatus(new Path(files[i].getPath()));\n          String cacheId=DistributedCache.makeRelative(files[i],conf);\n          String cachePath=TaskTracker.getCacheSubdir() + Path.SEPARATOR + cacheId;\n          if (lDirAlloc.ifExists(cachePath,conf)) {\n            localPath=lDirAlloc.getLocalPathToRead(cachePath,conf);\n          }\n else {\n            localPath=lDirAlloc.getLocalPathForWrite(cachePath,fileStatus.getLen(),conf);\n          }\n          baseDir=localPath.toString().replace(cacheId,\"\");\n          p[i]=DistributedCache.getLocalCache(files[i],conf,new Path(baseDir),fileStatus,false,Long.parseLong(fileTimestamps[i]),new Path(workDir.getAbsolutePath()),false);\n        }\n        DistributedCache.setLocalFiles(conf,stringifyPathArray(p));\n      }\n      Path localTaskFile=new Path(t.getJobFile());\n      FileSystem localFs=FileSystem.getLocal(conf);\n      localFs.delete(localTaskFile,true);\n      OutputStream out=localFs.create(localTaskFile);\n      try {\n        conf.writeXml(out);\n      }\n  finally {\n        out.close();\n      }\n    }\n    if (!prepare()) {\n      return;\n    }\n    String sep=System.getProperty(\"path.separator\");\n    StringBuffer classPath=new StringBuffer();\n    classPath.append(System.getProperty(\"java.class.path\"));\n    classPath.append(sep);\n    if (!workDir.mkdirs()) {\n      if (!workDir.isDirectory()) {\n        LOG.fatal(\"Mkdirs failed to create \" + workDir.toString());\n      }\n    }\n    String jar=conf.getJar();\n    if (jar != null) {\n      File[] libs=new File(jobCacheDir,\"lib\").listFiles();\n      if (libs != null) {\n        for (int i=0; i < libs.length; i++) {\n          classPath.append(sep);\n          classPath.append(libs[i]);\n        }\n      }\n      classPath.append(sep);\n      classPath.append(new File(jobCacheDir,\"classes\"));\n      classPath.append(sep);\n      classPath.append(jobCacheDir);\n    }\n    Path[] archiveClasspaths=DistributedCache.getArchiveClassPaths(conf);\n    if (archiveClasspaths != null && archives != null) {\n      Path[] localArchives=DistributedCache.getLocalCacheArchives(conf);\n      if (localArchives != null) {\n        for (int i=0; i < archives.length; i++) {\n          for (int j=0; j < archiveClasspaths.length; j++) {\n            if (archives[i].getPath().equals(archiveClasspaths[j].toString())) {\n              classPath.append(sep);\n              classPath.append(localArchives[i].toString());\n            }\n          }\n        }\n      }\n    }\n    Path[] fileClasspaths=DistributedCache.getFileClassPaths(conf);\n    if (fileClasspaths != null && files != null) {\n      Path[] localFiles=DistributedCache.getLocalCacheFiles(conf);\n      if (localFiles != null) {\n        for (int i=0; i < files.length; i++) {\n          for (int j=0; j < fileClasspaths.length; j++) {\n            if (files[i].getPath().equals(fileClasspaths[j].toString())) {\n              classPath.append(sep);\n              classPath.append(localFiles[i].toString());\n            }\n          }\n        }\n      }\n    }\n    classPath.append(sep);\n    classPath.append(workDir);\n    Vector<String> vargs=new Vector<String>(8);\n    File jvm=new File(new File(System.getProperty(\"java.home\"),\"bin\"),\"java\");\n    vargs.add(jvm.toString());\n    String javaOpts=conf.get(\"mapred.child.java.opts\",\"-Xmx200m\");\n    javaOpts=javaOpts.replace(\"@taskid@\",taskid.toString());\n    String[] javaOptsSplit=javaOpts.split(\" \");\n    String libraryPath=System.getProperty(\"java.library.path\");\n    if (libraryPath == null) {\n      libraryPath=workDir.getAbsolutePath();\n    }\n else {\n      libraryPath+=sep + workDir;\n    }\n    boolean hasUserLDPath=false;\n    for (int i=0; i < javaOptsSplit.length; i++) {\n      if (javaOptsSplit[i].startsWith(\"-Djava.library.path=\")) {\n        javaOptsSplit[i]+=sep + libraryPath;\n        hasUserLDPath=true;\n        break;\n      }\n    }\n    if (!hasUserLDPath) {\n      vargs.add(\"-Djava.library.path=\" + libraryPath);\n    }\n    for (int i=0; i < javaOptsSplit.length; i++) {\n      vargs.add(javaOptsSplit[i]);\n    }\n    String tmp=conf.get(\"mapred.child.tmp\",\"./tmp\");\n    Path tmpDir=new Path(tmp);\n    if (!tmpDir.isAbsolute()) {\n      tmpDir=new Path(workDir.toString(),tmp);\n    }\n    FileSystem localFs=FileSystem.getLocal(conf);\n    if (!localFs.mkdirs(tmpDir) && !localFs.getFileStatus(tmpDir).isDir()) {\n      throw new IOException(\"Mkdirs failed to create \" + tmpDir.toString());\n    }\n    vargs.add(\"-Djava.io.tmpdir=\" + tmpDir.toString());\n    vargs.add(\"-classpath\");\n    vargs.add(classPath.toString());\n    long logSize=TaskLog.getTaskLogLength(conf);\n    vargs.add(\"-Dhadoop.log.dir=\" + new File(System.getProperty(\"hadoop.log.dir\")).getAbsolutePath());\n    vargs.add(\"-Dhadoop.root.logger=INFO,TLA\");\n    vargs.add(\"-Dhadoop.tasklog.taskid=\" + taskid);\n    vargs.add(\"-Dhadoop.tasklog.totalLogFileSize=\" + logSize);\n    if (conf.getProfileEnabled()) {\n      if (conf.getProfileTaskRange(t.isMapTask()).isIncluded(t.getPartition())) {\n        File prof=TaskLog.getTaskLogFile(taskid,TaskLog.LogName.PROFILE);\n        vargs.add(String.format(conf.getProfileParams(),prof.toString()));\n      }\n    }\n    vargs.add(Child.class.getName());\n    InetSocketAddress address=tracker.getTaskTrackerReportAddress();\n    vargs.add(address.getAddress().getHostAddress());\n    vargs.add(Integer.toString(address.getPort()));\n    vargs.add(taskid.toString());\n    String pidFile=null;\n    if (tracker.isTaskMemoryManagerEnabled()) {\n      pidFile=lDirAlloc.getLocalPathForWrite((TaskTracker.getPidFilesSubdir() + Path.SEPARATOR + taskid),this.conf).toString();\n    }\n    String[] ulimitCmd=Shell.getUlimitMemoryCommand(conf);\n    List<String> setup=null;\n    if (ulimitCmd != null) {\n      setup=new ArrayList<String>();\n      for (      String arg : ulimitCmd) {\n        setup.add(arg);\n      }\n    }\n    File stdout=TaskLog.getTaskLogFile(taskid,TaskLog.LogName.STDOUT);\n    File stderr=TaskLog.getTaskLogFile(taskid,TaskLog.LogName.STDERR);\n    stdout.getParentFile().mkdirs();\n    tracker.getTaskTrackerInstrumentation().reportTaskLaunch(taskid,stdout,stderr);\n    Map<String,String> env=new HashMap<String,String>();\n    StringBuffer ldLibraryPath=new StringBuffer();\n    ldLibraryPath.append(workDir.toString());\n    String oldLdLibraryPath=null;\n    oldLdLibraryPath=System.getenv(\"LD_LIBRARY_PATH\");\n    if (oldLdLibraryPath != null) {\n      ldLibraryPath.append(sep);\n      ldLibraryPath.append(oldLdLibraryPath);\n    }\n    env.put(\"LD_LIBRARY_PATH\",ldLibraryPath.toString());\n    jvmManager.launchJvm(this,jvmManager.constructJvmEnv(setup,vargs,stdout,stderr,logSize,workDir,env,pidFile,conf));\nsynchronized (lock) {\n      while (!done) {\n        lock.wait();\n      }\n    }\n    tracker.getTaskTrackerInstrumentation().reportTaskEnd(t.getTaskID());\n    if (exitCodeSet) {\n      if (!killed && exitCode != 0) {\n        if (exitCode == 65) {\n          tracker.getTaskTrackerInstrumentation().taskFailedPing(t.getTaskID());\n        }\n        throw new IOException(\"Task process exit with nonzero status of \" + exitCode + \".\");\n      }\n    }\n  }\n catch (  FSError e) {\n    LOG.fatal(\"FSError\",e);\n    try {\n      tracker.fsError(t.getTaskID(),e.getMessage());\n    }\n catch (    IOException ie) {\n      LOG.fatal(t.getTaskID() + \" reporting FSError\",ie);\n    }\n  }\ncatch (  Throwable throwable) {\n    LOG.warn(t.getTaskID() + \" Child Error\",throwable);\n    ByteArrayOutputStream baos=new ByteArrayOutputStream();\n    throwable.printStackTrace(new PrintStream(baos));\n    try {\n      tracker.reportDiagnosticInfo(t.getTaskID(),baos.toString());\n    }\n catch (    IOException e) {\n      LOG.warn(t.getTaskID() + \" Reporting Diagnostics\",e);\n    }\n  }\n finally {\n    try {\n      URI[] archives=DistributedCache.getCacheArchives(conf);\n      URI[] files=DistributedCache.getCacheFiles(conf);\n      if (archives != null) {\n        for (int i=0; i < archives.length; i++) {\n          DistributedCache.releaseCache(archives[i],conf);\n        }\n      }\n      if (files != null) {\n        for (int i=0; i < files.length; i++) {\n          DistributedCache.releaseCache(files[i],conf);\n        }\n      }\n    }\n catch (    IOException ie) {\n      LOG.warn(\"Error releasing caches : Cache files might not have been cleaned up\");\n    }\n    tracker.reportTaskFinished(t.getTaskID(),false);\n  }\n}\n",
                "getTracker": "{\n  return tracker;\n}\n",
                "kill": "{\n  killed=true;\n  jvmManager.taskKilled(this);\n  signalDone();\n}\n",
                "close": "{\n}\n",
                "stringifyPathArray": "{\n  if (p == null) {\n    return null;\n  }\n  StringBuffer str=new StringBuffer(p[0].toString());\n  for (int i=1; i < p.length; i++) {\n    str.append(\",\");\n    str.append(p[i].toString());\n  }\n  return str.toString();\n}\n",
                "getTask": "{\n  return t;\n}\n",
                "TaskRunner": "{\n  this.tip=tip;\n  this.t=tip.getTask();\n  this.tracker=tracker;\n  this.conf=conf;\n  this.mapOutputFile=new MapOutputFile(t.getJobID());\n  this.mapOutputFile.setConf(conf);\n  this.jvmManager=tracker.getJvmManagerInstance();\n}\n"
            },
            "className": "org.apache.hadoop.mapred.TaskRunner",
            "body": "class TaskRunner{public static final Log LOG=LogFactory.getLog(TaskRunner.class); volatile boolean killed=false; private TaskTracker.TaskInProgress tip; private Task t; private Object lock=new Object(); private volatile boolean done=false; private int exitCode=-1; private boolean exitCodeSet=false; private TaskTracker tracker; protected JobConf conf; JvmManager jvmManager; protected MapOutputFile mapOutputFile; public TaskRunner(TaskTracker.TaskInProgress tip,TaskTracker tracker,JobConf conf){} public Task getTask(){} public TaskTracker.TaskInProgress getTaskInProgress(){} public TaskTracker getTracker(){} public boolean prepare() throws IOException {} public void close() throws IOException {} private static String stringifyPathArray(Path[] p){} @Override public final void run(){} public static void setupWorkDir(JobConf conf) throws IOException {} public void kill(){} public void signalDone(){} public void setExitCode(int exitCode){}  }",
            "classAnnotation": "/** \n * Base class that runs a task in a separate process.  Tasks are run in a separate process in order to isolate the map/reduce system code from bugs in user supplied map and reduce functions.\n */\n",
            "fullmethod": {
                "prepare": "/** \n * Called to assemble this task's input.  This method is run in the parent process before the child is spawned.  It should not execute user code, only system code. \n */\npublic boolean prepare() throws IOException {\n  return true;\n}\n",
                "signalDone": "public void signalDone(){\nsynchronized (lock) {\n    done=true;\n    lock.notify();\n  }\n}\n",
                "getTaskInProgress": "public TaskTracker.TaskInProgress getTaskInProgress(){\n  return tip;\n}\n",
                "setupWorkDir": "public static void setupWorkDir(JobConf conf) throws IOException {\n  File workDir=new File(\".\").getAbsoluteFile();\n  FileUtil.fullyDelete(workDir);\n  if (DistributedCache.getSymlink(conf)) {\n    URI[] archives=DistributedCache.getCacheArchives(conf);\n    URI[] files=DistributedCache.getCacheFiles(conf);\n    Path[] localArchives=DistributedCache.getLocalCacheArchives(conf);\n    Path[] localFiles=DistributedCache.getLocalCacheFiles(conf);\n    if (archives != null) {\n      for (int i=0; i < archives.length; i++) {\n        String link=archives[i].getFragment();\n        if (link != null) {\n          link=workDir.toString() + Path.SEPARATOR + link;\n          File flink=new File(link);\n          if (!flink.exists()) {\n            FileUtil.symLink(localArchives[i].toString(),link);\n          }\n        }\n      }\n    }\n    if (files != null) {\n      for (int i=0; i < files.length; i++) {\n        String link=files[i].getFragment();\n        if (link != null) {\n          link=workDir.toString() + Path.SEPARATOR + link;\n          File flink=new File(link);\n          if (!flink.exists()) {\n            FileUtil.symLink(localFiles[i].toString(),link);\n          }\n        }\n      }\n    }\n  }\n  File jobCacheDir=null;\n  if (conf.getJar() != null) {\n    jobCacheDir=new File(new Path(conf.getJar()).getParent().toString());\n  }\n  try {\n    DistributedCache.createAllSymlink(conf,jobCacheDir,workDir);\n  }\n catch (  IOException ie) {\n    LOG.warn(StringUtils.stringifyException(ie));\n  }\n  String tmp=conf.get(\"mapred.child.tmp\",\"./tmp\");\n  Path tmpDir=new Path(tmp);\n  if (!tmpDir.isAbsolute()) {\n    tmpDir=new Path(workDir.toString(),tmp);\n    FileSystem localFs=FileSystem.getLocal(conf);\n    if (!localFs.mkdirs(tmpDir) && !localFs.getFileStatus(tmpDir).isDir()) {\n      throw new IOException(\"Mkdirs failed to create \" + tmpDir.toString());\n    }\n  }\n}\n",
                "setExitCode": "public void setExitCode(int exitCode){\n  this.exitCodeSet=true;\n  this.exitCode=exitCode;\n}\n",
                "run": "@Override public final void run(){\n  try {\n    TaskAttemptID taskid=t.getTaskID();\n    LocalDirAllocator lDirAlloc=new LocalDirAllocator(\"mapred.local.dir\");\n    File jobCacheDir=null;\n    if (conf.getJar() != null) {\n      jobCacheDir=new File(new Path(conf.getJar()).getParent().toString());\n    }\n    File workDir=new File(lDirAlloc.getLocalPathToRead(TaskTracker.getJobCacheSubdir() + Path.SEPARATOR + t.getJobID()+ Path.SEPARATOR+ t.getTaskID()+ Path.SEPARATOR+ MRConstants.WORKDIR,conf).toString());\n    URI[] archives=DistributedCache.getCacheArchives(conf);\n    URI[] files=DistributedCache.getCacheFiles(conf);\n    FileStatus fileStatus;\n    FileSystem fileSystem;\n    Path localPath;\n    String baseDir;\n    if ((archives != null) || (files != null)) {\n      if (archives != null) {\n        String[] archivesTimestamps=DistributedCache.getArchiveTimestamps(conf);\n        Path[] p=new Path[archives.length];\n        for (int i=0; i < archives.length; i++) {\n          fileSystem=FileSystem.get(archives[i],conf);\n          fileStatus=fileSystem.getFileStatus(new Path(archives[i].getPath()));\n          String cacheId=DistributedCache.makeRelative(archives[i],conf);\n          String cachePath=TaskTracker.getCacheSubdir() + Path.SEPARATOR + cacheId;\n          if (lDirAlloc.ifExists(cachePath,conf)) {\n            localPath=lDirAlloc.getLocalPathToRead(cachePath,conf);\n          }\n else {\n            localPath=lDirAlloc.getLocalPathForWrite(cachePath,fileStatus.getLen(),conf);\n          }\n          baseDir=localPath.toString().replace(cacheId,\"\");\n          p[i]=DistributedCache.getLocalCache(archives[i],conf,new Path(baseDir),fileStatus,true,Long.parseLong(archivesTimestamps[i]),new Path(workDir.getAbsolutePath()),false);\n        }\n        DistributedCache.setLocalArchives(conf,stringifyPathArray(p));\n      }\n      if ((files != null)) {\n        String[] fileTimestamps=DistributedCache.getFileTimestamps(conf);\n        Path[] p=new Path[files.length];\n        for (int i=0; i < files.length; i++) {\n          fileSystem=FileSystem.get(files[i],conf);\n          fileStatus=fileSystem.getFileStatus(new Path(files[i].getPath()));\n          String cacheId=DistributedCache.makeRelative(files[i],conf);\n          String cachePath=TaskTracker.getCacheSubdir() + Path.SEPARATOR + cacheId;\n          if (lDirAlloc.ifExists(cachePath,conf)) {\n            localPath=lDirAlloc.getLocalPathToRead(cachePath,conf);\n          }\n else {\n            localPath=lDirAlloc.getLocalPathForWrite(cachePath,fileStatus.getLen(),conf);\n          }\n          baseDir=localPath.toString().replace(cacheId,\"\");\n          p[i]=DistributedCache.getLocalCache(files[i],conf,new Path(baseDir),fileStatus,false,Long.parseLong(fileTimestamps[i]),new Path(workDir.getAbsolutePath()),false);\n        }\n        DistributedCache.setLocalFiles(conf,stringifyPathArray(p));\n      }\n      Path localTaskFile=new Path(t.getJobFile());\n      FileSystem localFs=FileSystem.getLocal(conf);\n      localFs.delete(localTaskFile,true);\n      OutputStream out=localFs.create(localTaskFile);\n      try {\n        conf.writeXml(out);\n      }\n  finally {\n        out.close();\n      }\n    }\n    if (!prepare()) {\n      return;\n    }\n    String sep=System.getProperty(\"path.separator\");\n    StringBuffer classPath=new StringBuffer();\n    classPath.append(System.getProperty(\"java.class.path\"));\n    classPath.append(sep);\n    if (!workDir.mkdirs()) {\n      if (!workDir.isDirectory()) {\n        LOG.fatal(\"Mkdirs failed to create \" + workDir.toString());\n      }\n    }\n    String jar=conf.getJar();\n    if (jar != null) {\n      File[] libs=new File(jobCacheDir,\"lib\").listFiles();\n      if (libs != null) {\n        for (int i=0; i < libs.length; i++) {\n          classPath.append(sep);\n          classPath.append(libs[i]);\n        }\n      }\n      classPath.append(sep);\n      classPath.append(new File(jobCacheDir,\"classes\"));\n      classPath.append(sep);\n      classPath.append(jobCacheDir);\n    }\n    Path[] archiveClasspaths=DistributedCache.getArchiveClassPaths(conf);\n    if (archiveClasspaths != null && archives != null) {\n      Path[] localArchives=DistributedCache.getLocalCacheArchives(conf);\n      if (localArchives != null) {\n        for (int i=0; i < archives.length; i++) {\n          for (int j=0; j < archiveClasspaths.length; j++) {\n            if (archives[i].getPath().equals(archiveClasspaths[j].toString())) {\n              classPath.append(sep);\n              classPath.append(localArchives[i].toString());\n            }\n          }\n        }\n      }\n    }\n    Path[] fileClasspaths=DistributedCache.getFileClassPaths(conf);\n    if (fileClasspaths != null && files != null) {\n      Path[] localFiles=DistributedCache.getLocalCacheFiles(conf);\n      if (localFiles != null) {\n        for (int i=0; i < files.length; i++) {\n          for (int j=0; j < fileClasspaths.length; j++) {\n            if (files[i].getPath().equals(fileClasspaths[j].toString())) {\n              classPath.append(sep);\n              classPath.append(localFiles[i].toString());\n            }\n          }\n        }\n      }\n    }\n    classPath.append(sep);\n    classPath.append(workDir);\n    Vector<String> vargs=new Vector<String>(8);\n    File jvm=new File(new File(System.getProperty(\"java.home\"),\"bin\"),\"java\");\n    vargs.add(jvm.toString());\n    String javaOpts=conf.get(\"mapred.child.java.opts\",\"-Xmx200m\");\n    javaOpts=javaOpts.replace(\"@taskid@\",taskid.toString());\n    String[] javaOptsSplit=javaOpts.split(\" \");\n    String libraryPath=System.getProperty(\"java.library.path\");\n    if (libraryPath == null) {\n      libraryPath=workDir.getAbsolutePath();\n    }\n else {\n      libraryPath+=sep + workDir;\n    }\n    boolean hasUserLDPath=false;\n    for (int i=0; i < javaOptsSplit.length; i++) {\n      if (javaOptsSplit[i].startsWith(\"-Djava.library.path=\")) {\n        javaOptsSplit[i]+=sep + libraryPath;\n        hasUserLDPath=true;\n        break;\n      }\n    }\n    if (!hasUserLDPath) {\n      vargs.add(\"-Djava.library.path=\" + libraryPath);\n    }\n    for (int i=0; i < javaOptsSplit.length; i++) {\n      vargs.add(javaOptsSplit[i]);\n    }\n    String tmp=conf.get(\"mapred.child.tmp\",\"./tmp\");\n    Path tmpDir=new Path(tmp);\n    if (!tmpDir.isAbsolute()) {\n      tmpDir=new Path(workDir.toString(),tmp);\n    }\n    FileSystem localFs=FileSystem.getLocal(conf);\n    if (!localFs.mkdirs(tmpDir) && !localFs.getFileStatus(tmpDir).isDir()) {\n      throw new IOException(\"Mkdirs failed to create \" + tmpDir.toString());\n    }\n    vargs.add(\"-Djava.io.tmpdir=\" + tmpDir.toString());\n    vargs.add(\"-classpath\");\n    vargs.add(classPath.toString());\n    long logSize=TaskLog.getTaskLogLength(conf);\n    vargs.add(\"-Dhadoop.log.dir=\" + new File(System.getProperty(\"hadoop.log.dir\")).getAbsolutePath());\n    vargs.add(\"-Dhadoop.root.logger=INFO,TLA\");\n    vargs.add(\"-Dhadoop.tasklog.taskid=\" + taskid);\n    vargs.add(\"-Dhadoop.tasklog.totalLogFileSize=\" + logSize);\n    if (conf.getProfileEnabled()) {\n      if (conf.getProfileTaskRange(t.isMapTask()).isIncluded(t.getPartition())) {\n        File prof=TaskLog.getTaskLogFile(taskid,TaskLog.LogName.PROFILE);\n        vargs.add(String.format(conf.getProfileParams(),prof.toString()));\n      }\n    }\n    vargs.add(Child.class.getName());\n    InetSocketAddress address=tracker.getTaskTrackerReportAddress();\n    vargs.add(address.getAddress().getHostAddress());\n    vargs.add(Integer.toString(address.getPort()));\n    vargs.add(taskid.toString());\n    String pidFile=null;\n    if (tracker.isTaskMemoryManagerEnabled()) {\n      pidFile=lDirAlloc.getLocalPathForWrite((TaskTracker.getPidFilesSubdir() + Path.SEPARATOR + taskid),this.conf).toString();\n    }\n    String[] ulimitCmd=Shell.getUlimitMemoryCommand(conf);\n    List<String> setup=null;\n    if (ulimitCmd != null) {\n      setup=new ArrayList<String>();\n      for (      String arg : ulimitCmd) {\n        setup.add(arg);\n      }\n    }\n    File stdout=TaskLog.getTaskLogFile(taskid,TaskLog.LogName.STDOUT);\n    File stderr=TaskLog.getTaskLogFile(taskid,TaskLog.LogName.STDERR);\n    stdout.getParentFile().mkdirs();\n    tracker.getTaskTrackerInstrumentation().reportTaskLaunch(taskid,stdout,stderr);\n    Map<String,String> env=new HashMap<String,String>();\n    StringBuffer ldLibraryPath=new StringBuffer();\n    ldLibraryPath.append(workDir.toString());\n    String oldLdLibraryPath=null;\n    oldLdLibraryPath=System.getenv(\"LD_LIBRARY_PATH\");\n    if (oldLdLibraryPath != null) {\n      ldLibraryPath.append(sep);\n      ldLibraryPath.append(oldLdLibraryPath);\n    }\n    env.put(\"LD_LIBRARY_PATH\",ldLibraryPath.toString());\n    jvmManager.launchJvm(this,jvmManager.constructJvmEnv(setup,vargs,stdout,stderr,logSize,workDir,env,pidFile,conf));\nsynchronized (lock) {\n      while (!done) {\n        lock.wait();\n      }\n    }\n    tracker.getTaskTrackerInstrumentation().reportTaskEnd(t.getTaskID());\n    if (exitCodeSet) {\n      if (!killed && exitCode != 0) {\n        if (exitCode == 65) {\n          tracker.getTaskTrackerInstrumentation().taskFailedPing(t.getTaskID());\n        }\n        throw new IOException(\"Task process exit with nonzero status of \" + exitCode + \".\");\n      }\n    }\n  }\n catch (  FSError e) {\n    LOG.fatal(\"FSError\",e);\n    try {\n      tracker.fsError(t.getTaskID(),e.getMessage());\n    }\n catch (    IOException ie) {\n      LOG.fatal(t.getTaskID() + \" reporting FSError\",ie);\n    }\n  }\ncatch (  Throwable throwable) {\n    LOG.warn(t.getTaskID() + \" Child Error\",throwable);\n    ByteArrayOutputStream baos=new ByteArrayOutputStream();\n    throwable.printStackTrace(new PrintStream(baos));\n    try {\n      tracker.reportDiagnosticInfo(t.getTaskID(),baos.toString());\n    }\n catch (    IOException e) {\n      LOG.warn(t.getTaskID() + \" Reporting Diagnostics\",e);\n    }\n  }\n finally {\n    try {\n      URI[] archives=DistributedCache.getCacheArchives(conf);\n      URI[] files=DistributedCache.getCacheFiles(conf);\n      if (archives != null) {\n        for (int i=0; i < archives.length; i++) {\n          DistributedCache.releaseCache(archives[i],conf);\n        }\n      }\n      if (files != null) {\n        for (int i=0; i < files.length; i++) {\n          DistributedCache.releaseCache(files[i],conf);\n        }\n      }\n    }\n catch (    IOException ie) {\n      LOG.warn(\"Error releasing caches : Cache files might not have been cleaned up\");\n    }\n    tracker.reportTaskFinished(t.getTaskID(),false);\n  }\n}\n",
                "getTracker": "public TaskTracker getTracker(){\n  return tracker;\n}\n",
                "kill": "/** \n * Kill the child process\n */\npublic void kill(){\n  killed=true;\n  jvmManager.taskKilled(this);\n  signalDone();\n}\n",
                "close": "/** \n * Called when this task's output is no longer needed. This method is run in the parent process after the child exits.  It should not execute user code, only system code.\n */\npublic void close() throws IOException {\n}\n",
                "stringifyPathArray": "private static String stringifyPathArray(Path[] p){\n  if (p == null) {\n    return null;\n  }\n  StringBuffer str=new StringBuffer(p[0].toString());\n  for (int i=1; i < p.length; i++) {\n    str.append(\",\");\n    str.append(p[i].toString());\n  }\n  return str.toString();\n}\n",
                "getTask": "public Task getTask(){\n  return t;\n}\n",
                "TaskRunner": "public TaskRunner(TaskTracker.TaskInProgress tip,TaskTracker tracker,JobConf conf){\n  this.tip=tip;\n  this.t=tip.getTask();\n  this.tracker=tracker;\n  this.conf=conf;\n  this.mapOutputFile=new MapOutputFile(t.getJobID());\n  this.mapOutputFile.setConf(conf);\n  this.jvmManager=tracker.getJvmManagerInstance();\n}\n"
            },
            "deps": [
                "org.apache.hadoop.mapred.MapTaskRunner Call org.apache.hadoop.mapred.TaskRunner",
                "org.apache.hadoop.mapred.MapTaskRunner Extend org.apache.hadoop.mapred.TaskRunner",
                "org.apache.hadoop.mapred.MapTaskRunner Use org.apache.hadoop.mapred.TaskRunner",
                "org.apache.hadoop.mapred.Child Call org.apache.hadoop.mapred.TaskRunner",
                "org.apache.hadoop.mapred.Child Use org.apache.hadoop.mapred.TaskRunner",
                "org.apache.hadoop.mapred.TaskRunner Call org.apache.hadoop.util.StringUtils",
                "org.apache.hadoop.mapred.TaskRunner Use org.apache.hadoop.util.StringUtils",
                "org.apache.hadoop.mapred.TaskRunner Call org.apache.hadoop.mapred.TaskAttemptID",
                "org.apache.hadoop.mapred.TaskRunner Contain org.apache.hadoop.mapred.TaskAttemptID",
                "org.apache.hadoop.mapred.TaskRunner Call org.apache.hadoop.mapred.TaskTrackerInstrumentation",
                "org.apache.hadoop.mapred.TaskRunner Call org.apache.hadoop.mapred.MapOutputFile",
                "org.apache.hadoop.mapred.TaskRunner Use org.apache.hadoop.mapred.MapOutputFile",
                "org.apache.hadoop.mapred.TaskRunner Create org.apache.hadoop.mapred.MapOutputFile",
                "org.apache.hadoop.mapred.TaskRunner Contain org.apache.hadoop.mapred.MapOutputFile",
                "org.apache.hadoop.mapred.TaskRunner Call org.apache.hadoop.fs.FileStatus",
                "org.apache.hadoop.mapred.TaskRunner Use org.apache.hadoop.fs.FileStatus",
                "org.apache.hadoop.mapred.TaskRunner Contain org.apache.hadoop.fs.FileStatus",
                "org.apache.hadoop.mapred.TaskRunner Call org.apache.hadoop.fs.Path",
                "org.apache.hadoop.mapred.TaskRunner Use org.apache.hadoop.fs.Path",
                "org.apache.hadoop.mapred.TaskRunner Create org.apache.hadoop.fs.Path",
                "org.apache.hadoop.mapred.TaskRunner Parameter org.apache.hadoop.fs.Path",
                "org.apache.hadoop.mapred.TaskRunner Contain org.apache.hadoop.fs.Path",
                "org.apache.hadoop.mapred.TaskRunner Call org.apache.hadoop.mapred.JobConf",
                "org.apache.hadoop.mapred.TaskRunner Use org.apache.hadoop.mapred.JobConf",
                "org.apache.hadoop.mapred.TaskRunner Parameter org.apache.hadoop.mapred.JobConf",
                "org.apache.hadoop.mapred.TaskRunner Contain org.apache.hadoop.mapred.JobConf",
                "org.apache.hadoop.mapred.TaskRunner Use org.apache.hadoop.mapred.Child",
                "org.apache.hadoop.mapred.ReduceTaskRunner Call org.apache.hadoop.mapred.TaskRunner",
                "org.apache.hadoop.mapred.ReduceTaskRunner Extend org.apache.hadoop.mapred.TaskRunner",
                "org.apache.hadoop.mapred.ReduceTaskRunner Use org.apache.hadoop.mapred.TaskRunner"
            ],
            "directory": "org.apache.hadoop.mapred"
        },
        "org.apache.hadoop.hdfs.server.datanode.DataXceiver": {
            "method": {
                "getBlockChecksum": "{\n  final Block block=new Block(in.readLong(),0,in.readLong());\n  DataOutputStream out=null;\n  final MetaDataInputStream metadataIn=datanode.data.getMetaDataInputStream(block);\n  final DataInputStream checksumIn=new DataInputStream(new BufferedInputStream(metadataIn,BUFFER_SIZE));\n  try {\n    final BlockMetadataHeader header=BlockMetadataHeader.readHeader(checksumIn);\n    final DataChecksum checksum=header.getChecksum();\n    final int bytesPerCRC=checksum.getBytesPerChecksum();\n    final long crcPerBlock=(metadataIn.getLength() - BlockMetadataHeader.getHeaderSize()) / checksum.getChecksumSize();\n    final MD5Hash md5=MD5Hash.digest(checksumIn);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"block=\" + block + \", bytesPerCRC=\"+ bytesPerCRC+ \", crcPerBlock=\"+ crcPerBlock+ \", md5=\"+ md5);\n    }\n    out=new DataOutputStream(NetUtils.getOutputStream(s,datanode.socketWriteTimeout));\n    out.writeShort(DataTransferProtocol.OP_STATUS_SUCCESS);\n    out.writeInt(bytesPerCRC);\n    out.writeLong(crcPerBlock);\n    md5.write(out);\n    out.flush();\n  }\n  finally {\n    IOUtils.closeStream(out);\n    IOUtils.closeStream(checksumIn);\n    IOUtils.closeStream(metadataIn);\n  }\n}\n",
                "readBlock": "{\n  long blockId=in.readLong();\n  Block block=new Block(blockId,0,in.readLong());\n  long startOffset=in.readLong();\n  long length=in.readLong();\n  String clientName=Text.readString(in);\n  OutputStream baseStream=NetUtils.getOutputStream(s,datanode.socketWriteTimeout);\n  DataOutputStream out=new DataOutputStream(new BufferedOutputStream(baseStream,SMALL_BUFFER_SIZE));\n  BlockSender blockSender=null;\n  final String clientTraceFmt=clientName.length() > 0 && ClientTraceLog.isInfoEnabled() ? String.format(DN_CLIENTTRACE_FORMAT,localAddress,remoteAddress,\"%d\",\"HDFS_READ\",clientName,datanode.dnRegistration.getStorageID(),block) : datanode.dnRegistration + \" Served block \" + block+ \" to \"+ s.getInetAddress();\n  try {\n    try {\n      blockSender=new BlockSender(block,startOffset,length,true,true,false,datanode,clientTraceFmt);\n    }\n catch (    IOException e) {\n      out.writeShort(DataTransferProtocol.OP_STATUS_ERROR);\n      throw e;\n    }\n    out.writeShort(DataTransferProtocol.OP_STATUS_SUCCESS);\n    long read=blockSender.sendBlock(out,baseStream,null);\n    if (blockSender.isBlockReadFully()) {\n      try {\n        if (in.readShort() == DataTransferProtocol.OP_STATUS_CHECKSUM_OK && datanode.blockScanner != null) {\n          datanode.blockScanner.verifiedByClient(block);\n        }\n      }\n catch (      IOException ignored) {\n      }\n    }\n    datanode.myMetrics.bytesRead.inc((int)read);\n    datanode.myMetrics.blocksRead.inc();\n  }\n catch (  SocketException ignored) {\n    datanode.myMetrics.blocksRead.inc();\n  }\ncatch (  IOException ioe) {\n    LOG.warn(datanode.dnRegistration + \":Got exception while serving \" + block+ \" to \"+ s.getInetAddress()+ \":\\n\"+ StringUtils.stringifyException(ioe));\n    throw ioe;\n  }\n finally {\n    IOUtils.closeStream(out);\n    IOUtils.closeStream(blockSender);\n  }\n}\n",
                "DataXceiver": "{\n  this.s=s;\n  this.datanode=datanode;\n  this.dataXceiverServer=dataXceiverServer;\n  remoteAddress=s.getRemoteSocketAddress().toString();\n  localAddress=s.getLocalSocketAddress().toString();\n  LOG.debug(\"Number of active connections is: \" + datanode.getXceiverCount());\n}\n",
                "replaceBlock": "{\n  long blockId=in.readLong();\n  Block block=new Block(blockId,dataXceiverServer.estimateBlockSize,in.readLong());\n  String sourceID=Text.readString(in);\n  DatanodeInfo proxySource=new DatanodeInfo();\n  proxySource.readFields(in);\n  if (!dataXceiverServer.balanceThrottler.acquire()) {\n    LOG.warn(\"Not able to receive block \" + blockId + \" from \"+ s.getRemoteSocketAddress()+ \" because threads quota is exceeded.\");\n    sendResponse(s,(short)DataTransferProtocol.OP_STATUS_ERROR,datanode.socketWriteTimeout);\n    return;\n  }\n  Socket proxySock=null;\n  DataOutputStream proxyOut=null;\n  short opStatus=DataTransferProtocol.OP_STATUS_SUCCESS;\n  BlockReceiver blockReceiver=null;\n  DataInputStream proxyReply=null;\n  try {\n    InetSocketAddress proxyAddr=NetUtils.createSocketAddr(proxySource.getName());\n    proxySock=datanode.newSocket();\n    proxySock.connect(proxyAddr,datanode.socketTimeout);\n    proxySock.setSoTimeout(datanode.socketTimeout);\n    OutputStream baseStream=NetUtils.getOutputStream(proxySock,datanode.socketWriteTimeout);\n    proxyOut=new DataOutputStream(new BufferedOutputStream(baseStream,SMALL_BUFFER_SIZE));\n    proxyOut.writeShort(DataTransferProtocol.DATA_TRANSFER_VERSION);\n    proxyOut.writeByte(DataTransferProtocol.OP_COPY_BLOCK);\n    proxyOut.writeLong(block.getBlockId());\n    proxyOut.writeLong(block.getGenerationStamp());\n    proxyOut.flush();\n    proxyReply=new DataInputStream(new BufferedInputStream(NetUtils.getInputStream(proxySock),BUFFER_SIZE));\n    blockReceiver=new BlockReceiver(block,proxyReply,proxySock.getRemoteSocketAddress().toString(),proxySock.getLocalSocketAddress().toString(),false,\"\",null,datanode);\n    blockReceiver.receiveBlock(null,null,null,null,dataXceiverServer.balanceThrottler,-1);\n    datanode.notifyNamenodeReceivedBlock(block,sourceID);\n    LOG.info(\"Moved block \" + block + \" from \"+ s.getRemoteSocketAddress());\n  }\n catch (  IOException ioe) {\n    opStatus=DataTransferProtocol.OP_STATUS_ERROR;\n    throw ioe;\n  }\n finally {\n    if (opStatus == DataTransferProtocol.OP_STATUS_SUCCESS) {\n      try {\n        proxyReply.readChar();\n      }\n catch (      IOException ignored) {\n      }\n    }\n    dataXceiverServer.balanceThrottler.release();\n    try {\n      sendResponse(s,opStatus,datanode.socketWriteTimeout);\n    }\n catch (    IOException ioe) {\n      LOG.warn(\"Error writing reply back to \" + s.getRemoteSocketAddress());\n    }\n    IOUtils.closeStream(proxyOut);\n    IOUtils.closeStream(blockReceiver);\n    IOUtils.closeStream(proxyReply);\n  }\n}\n",
                "writeBlock": "{\n  DatanodeInfo srcDataNode=null;\n  LOG.debug(\"writeBlock receive buf size \" + s.getReceiveBufferSize() + \" tcp no delay \"+ s.getTcpNoDelay());\n  Block block=new Block(in.readLong(),dataXceiverServer.estimateBlockSize,in.readLong());\n  LOG.info(\"Receiving block \" + block + \" src: \"+ remoteAddress+ \" dest: \"+ localAddress);\n  int pipelineSize=in.readInt();\n  boolean isRecovery=in.readBoolean();\n  String client=Text.readString(in);\n  boolean hasSrcDataNode=in.readBoolean();\n  if (hasSrcDataNode) {\n    srcDataNode=new DatanodeInfo();\n    srcDataNode.readFields(in);\n  }\n  int numTargets=in.readInt();\n  if (numTargets < 0) {\n    throw new IOException(\"Mislabelled incoming datastream.\");\n  }\n  DatanodeInfo targets[]=new DatanodeInfo[numTargets];\n  for (int i=0; i < targets.length; i++) {\n    DatanodeInfo tmp=new DatanodeInfo();\n    tmp.readFields(in);\n    targets[i]=tmp;\n  }\n  DataOutputStream mirrorOut=null;\n  DataInputStream mirrorIn=null;\n  DataOutputStream replyOut=null;\n  Socket mirrorSock=null;\n  BlockReceiver blockReceiver=null;\n  String mirrorNode=null;\n  String firstBadLink=\"\";\n  try {\n    blockReceiver=new BlockReceiver(block,in,s.getRemoteSocketAddress().toString(),s.getLocalSocketAddress().toString(),isRecovery,client,srcDataNode,datanode);\n    replyOut=new DataOutputStream(NetUtils.getOutputStream(s,datanode.socketWriteTimeout));\n    if (targets.length > 0) {\n      InetSocketAddress mirrorTarget=null;\n      mirrorNode=targets[0].getName();\n      mirrorTarget=NetUtils.createSocketAddr(mirrorNode);\n      mirrorSock=datanode.newSocket();\n      try {\n        int timeoutValue=numTargets * datanode.socketTimeout;\n        int writeTimeout=datanode.socketWriteTimeout + (HdfsConstants.WRITE_TIMEOUT_EXTENSION * numTargets);\n        mirrorSock.connect(mirrorTarget,timeoutValue);\n        mirrorSock.setSoTimeout(timeoutValue);\n        mirrorSock.setSendBufferSize(DEFAULT_DATA_SOCKET_SIZE);\n        mirrorOut=new DataOutputStream(new BufferedOutputStream(NetUtils.getOutputStream(mirrorSock,writeTimeout),SMALL_BUFFER_SIZE));\n        mirrorIn=new DataInputStream(NetUtils.getInputStream(mirrorSock));\n        mirrorOut.writeShort(DataTransferProtocol.DATA_TRANSFER_VERSION);\n        mirrorOut.write(DataTransferProtocol.OP_WRITE_BLOCK);\n        mirrorOut.writeLong(block.getBlockId());\n        mirrorOut.writeLong(block.getGenerationStamp());\n        mirrorOut.writeInt(pipelineSize);\n        mirrorOut.writeBoolean(isRecovery);\n        Text.writeString(mirrorOut,client);\n        mirrorOut.writeBoolean(hasSrcDataNode);\n        if (hasSrcDataNode) {\n          srcDataNode.write(mirrorOut);\n        }\n        mirrorOut.writeInt(targets.length - 1);\n        for (int i=1; i < targets.length; i++) {\n          targets[i].write(mirrorOut);\n        }\n        blockReceiver.writeChecksumHeader(mirrorOut);\n        mirrorOut.flush();\n        if (client.length() != 0) {\n          firstBadLink=Text.readString(mirrorIn);\n          if (LOG.isDebugEnabled() || firstBadLink.length() > 0) {\n            LOG.info(\"Datanode \" + targets.length + \" got response for connect ack \"+ \" from downstream datanode with firstbadlink as \"+ firstBadLink);\n          }\n        }\n      }\n catch (      IOException e) {\n        if (client.length() != 0) {\n          Text.writeString(replyOut,mirrorNode);\n          replyOut.flush();\n        }\n        IOUtils.closeStream(mirrorOut);\n        mirrorOut=null;\n        IOUtils.closeStream(mirrorIn);\n        mirrorIn=null;\n        IOUtils.closeSocket(mirrorSock);\n        mirrorSock=null;\n        if (client.length() > 0) {\n          throw e;\n        }\n else {\n          LOG.info(datanode.dnRegistration + \":Exception transfering block \" + block+ \" to mirror \"+ mirrorNode+ \". continuing without the mirror.\\n\"+ StringUtils.stringifyException(e));\n        }\n      }\n    }\n    if (client.length() != 0) {\n      if (LOG.isDebugEnabled() || firstBadLink.length() > 0) {\n        LOG.info(\"Datanode \" + targets.length + \" forwarding connect ack to upstream firstbadlink is \"+ firstBadLink);\n      }\n      Text.writeString(replyOut,firstBadLink);\n      replyOut.flush();\n    }\n    String mirrorAddr=(mirrorSock == null) ? null : mirrorNode;\n    blockReceiver.receiveBlock(mirrorOut,mirrorIn,replyOut,mirrorAddr,null,targets.length);\n    if (client.length() == 0) {\n      datanode.notifyNamenodeReceivedBlock(block,DataNode.EMPTY_DEL_HINT);\n      LOG.info(\"Received block \" + block + \" src: \"+ remoteAddress+ \" dest: \"+ localAddress+ \" of size \"+ block.getNumBytes());\n    }\n    if (datanode.blockScanner != null) {\n      datanode.blockScanner.addBlock(block);\n    }\n  }\n catch (  IOException ioe) {\n    LOG.info(\"writeBlock \" + block + \" received exception \"+ ioe);\n    throw ioe;\n  }\n finally {\n    IOUtils.closeStream(mirrorOut);\n    IOUtils.closeStream(mirrorIn);\n    IOUtils.closeStream(replyOut);\n    IOUtils.closeSocket(mirrorSock);\n    IOUtils.closeStream(blockReceiver);\n  }\n}\n",
                "readMetadata": "{\n  Block block=new Block(in.readLong(),0,in.readLong());\n  MetaDataInputStream checksumIn=null;\n  DataOutputStream out=null;\n  try {\n    checksumIn=datanode.data.getMetaDataInputStream(block);\n    long fileSize=checksumIn.getLength();\n    if (fileSize >= 1L << 31 || fileSize <= 0) {\n      throw new IOException(\"Unexpected size for checksumFile of block\" + block);\n    }\n    byte[] buf=new byte[(int)fileSize];\n    IOUtils.readFully(checksumIn,buf,0,buf.length);\n    out=new DataOutputStream(NetUtils.getOutputStream(s,datanode.socketWriteTimeout));\n    out.writeByte(DataTransferProtocol.OP_STATUS_SUCCESS);\n    out.writeInt(buf.length);\n    out.write(buf);\n    out.writeInt(0);\n  }\n  finally {\n    IOUtils.closeStream(out);\n    IOUtils.closeStream(checksumIn);\n  }\n}\n",
                "run": "{\n  DataInputStream in=null;\n  try {\n    in=new DataInputStream(new BufferedInputStream(NetUtils.getInputStream(s),SMALL_BUFFER_SIZE));\n    short version=in.readShort();\n    if (version != DataTransferProtocol.DATA_TRANSFER_VERSION) {\n      throw new IOException(\"Version Mismatch\");\n    }\n    boolean local=s.getInetAddress().equals(s.getLocalAddress());\n    byte op=in.readByte();\n    int curXceiverCount=datanode.getXceiverCount();\n    if (curXceiverCount > dataXceiverServer.maxXceiverCount) {\n      throw new IOException(\"xceiverCount \" + curXceiverCount + \" exceeds the limit of concurrent xcievers \"+ dataXceiverServer.maxXceiverCount);\n    }\n    long startTime=DataNode.now();\nswitch (op) {\ncase DataTransferProtocol.OP_READ_BLOCK:\n      readBlock(in);\n    datanode.myMetrics.readBlockOp.inc(DataNode.now() - startTime);\n  if (local)   datanode.myMetrics.readsFromLocalClient.inc();\n else   datanode.myMetrics.readsFromRemoteClient.inc();\nbreak;\ncase DataTransferProtocol.OP_WRITE_BLOCK:\nwriteBlock(in);\ndatanode.myMetrics.writeBlockOp.inc(DataNode.now() - startTime);\nif (local) datanode.myMetrics.writesFromLocalClient.inc();\n else datanode.myMetrics.writesFromRemoteClient.inc();\nbreak;\ncase DataTransferProtocol.OP_READ_METADATA:\nreadMetadata(in);\ndatanode.myMetrics.readMetadataOp.inc(DataNode.now() - startTime);\nbreak;\ncase DataTransferProtocol.OP_REPLACE_BLOCK:\nreplaceBlock(in);\ndatanode.myMetrics.replaceBlockOp.inc(DataNode.now() - startTime);\nbreak;\ncase DataTransferProtocol.OP_COPY_BLOCK:\ncopyBlock(in);\ndatanode.myMetrics.copyBlockOp.inc(DataNode.now() - startTime);\nbreak;\ncase DataTransferProtocol.OP_BLOCK_CHECKSUM:\ngetBlockChecksum(in);\ndatanode.myMetrics.blockChecksumOp.inc(DataNode.now() - startTime);\nbreak;\ndefault :\nthrow new IOException(\"Unknown opcode \" + op + \" in data stream\");\n}\n}\n catch (Throwable t) {\nLOG.error(datanode.dnRegistration + \":DataXceiver\",t);\n}\n finally {\nLOG.debug(datanode.dnRegistration + \":Number of active connections is: \" + datanode.getXceiverCount());\nIOUtils.closeStream(in);\nIOUtils.closeSocket(s);\ndataXceiverServer.childSockets.remove(s);\n}\n}\n",
                "copyBlock": "{\n  long blockId=in.readLong();\n  Block block=new Block(blockId,0,in.readLong());\n  if (!dataXceiverServer.balanceThrottler.acquire()) {\n    LOG.info(\"Not able to copy block \" + blockId + \" to \"+ s.getRemoteSocketAddress()+ \" because threads quota is exceeded.\");\n    return;\n  }\n  BlockSender blockSender=null;\n  DataOutputStream reply=null;\n  boolean isOpSuccess=true;\n  try {\n    blockSender=new BlockSender(block,0,-1,false,false,false,datanode);\n    OutputStream baseStream=NetUtils.getOutputStream(s,datanode.socketWriteTimeout);\n    reply=new DataOutputStream(new BufferedOutputStream(baseStream,SMALL_BUFFER_SIZE));\n    long read=blockSender.sendBlock(reply,baseStream,dataXceiverServer.balanceThrottler);\n    datanode.myMetrics.bytesRead.inc((int)read);\n    datanode.myMetrics.blocksRead.inc();\n    LOG.info(\"Copied block \" + block + \" to \"+ s.getRemoteSocketAddress());\n  }\n catch (  IOException ioe) {\n    isOpSuccess=false;\n    throw ioe;\n  }\n finally {\n    dataXceiverServer.balanceThrottler.release();\n    if (isOpSuccess) {\n      try {\n        reply.writeChar('d');\n      }\n catch (      IOException ignored) {\n      }\n    }\n    IOUtils.closeStream(reply);\n    IOUtils.closeStream(blockSender);\n  }\n}\n",
                "sendResponse": "{\n  DataOutputStream reply=new DataOutputStream(NetUtils.getOutputStream(s,timeout));\n  try {\n    reply.writeShort(opStatus);\n    reply.flush();\n  }\n  finally {\n    IOUtils.closeStream(reply);\n  }\n}\n"
            },
            "className": "org.apache.hadoop.hdfs.server.datanode.DataXceiver",
            "body": "class DataXceiver{public static final Log LOG=DataNode.LOG; static final Log ClientTraceLog=DataNode.ClientTraceLog; Socket s; final String remoteAddress; final String localAddress; DataNode datanode; DataXceiverServer dataXceiverServer; public DataXceiver(Socket s,DataNode datanode,DataXceiverServer dataXceiverServer){} public void run(){} private void readBlock(DataInputStream in) throws IOException {} private void writeBlock(DataInputStream in) throws IOException {} void readMetadata(DataInputStream in) throws IOException {} void getBlockChecksum(DataInputStream in) throws IOException {} private void copyBlock(DataInputStream in) throws IOException {} private void replaceBlock(DataInputStream in) throws IOException {} private void sendResponse(Socket s,short opStatus,long timeout) throws IOException {}  }",
            "classAnnotation": "/** \n * Thread for processing incoming/outgoing data stream.\n */\n",
            "fullmethod": {
                "getBlockChecksum": "/** \n * Get block checksum (MD5 of CRC32).\n * @param in\n */\nvoid getBlockChecksum(DataInputStream in) throws IOException {\n  final Block block=new Block(in.readLong(),0,in.readLong());\n  DataOutputStream out=null;\n  final MetaDataInputStream metadataIn=datanode.data.getMetaDataInputStream(block);\n  final DataInputStream checksumIn=new DataInputStream(new BufferedInputStream(metadataIn,BUFFER_SIZE));\n  try {\n    final BlockMetadataHeader header=BlockMetadataHeader.readHeader(checksumIn);\n    final DataChecksum checksum=header.getChecksum();\n    final int bytesPerCRC=checksum.getBytesPerChecksum();\n    final long crcPerBlock=(metadataIn.getLength() - BlockMetadataHeader.getHeaderSize()) / checksum.getChecksumSize();\n    final MD5Hash md5=MD5Hash.digest(checksumIn);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"block=\" + block + \", bytesPerCRC=\"+ bytesPerCRC+ \", crcPerBlock=\"+ crcPerBlock+ \", md5=\"+ md5);\n    }\n    out=new DataOutputStream(NetUtils.getOutputStream(s,datanode.socketWriteTimeout));\n    out.writeShort(DataTransferProtocol.OP_STATUS_SUCCESS);\n    out.writeInt(bytesPerCRC);\n    out.writeLong(crcPerBlock);\n    md5.write(out);\n    out.flush();\n  }\n  finally {\n    IOUtils.closeStream(out);\n    IOUtils.closeStream(checksumIn);\n    IOUtils.closeStream(metadataIn);\n  }\n}\n",
                "readBlock": "/** \n * Read a block from the disk.\n * @param in The stream to read from\n * @throws IOException\n */\nprivate void readBlock(DataInputStream in) throws IOException {\n  long blockId=in.readLong();\n  Block block=new Block(blockId,0,in.readLong());\n  long startOffset=in.readLong();\n  long length=in.readLong();\n  String clientName=Text.readString(in);\n  OutputStream baseStream=NetUtils.getOutputStream(s,datanode.socketWriteTimeout);\n  DataOutputStream out=new DataOutputStream(new BufferedOutputStream(baseStream,SMALL_BUFFER_SIZE));\n  BlockSender blockSender=null;\n  final String clientTraceFmt=clientName.length() > 0 && ClientTraceLog.isInfoEnabled() ? String.format(DN_CLIENTTRACE_FORMAT,localAddress,remoteAddress,\"%d\",\"HDFS_READ\",clientName,datanode.dnRegistration.getStorageID(),block) : datanode.dnRegistration + \" Served block \" + block+ \" to \"+ s.getInetAddress();\n  try {\n    try {\n      blockSender=new BlockSender(block,startOffset,length,true,true,false,datanode,clientTraceFmt);\n    }\n catch (    IOException e) {\n      out.writeShort(DataTransferProtocol.OP_STATUS_ERROR);\n      throw e;\n    }\n    out.writeShort(DataTransferProtocol.OP_STATUS_SUCCESS);\n    long read=blockSender.sendBlock(out,baseStream,null);\n    if (blockSender.isBlockReadFully()) {\n      try {\n        if (in.readShort() == DataTransferProtocol.OP_STATUS_CHECKSUM_OK && datanode.blockScanner != null) {\n          datanode.blockScanner.verifiedByClient(block);\n        }\n      }\n catch (      IOException ignored) {\n      }\n    }\n    datanode.myMetrics.bytesRead.inc((int)read);\n    datanode.myMetrics.blocksRead.inc();\n  }\n catch (  SocketException ignored) {\n    datanode.myMetrics.blocksRead.inc();\n  }\ncatch (  IOException ioe) {\n    LOG.warn(datanode.dnRegistration + \":Got exception while serving \" + block+ \" to \"+ s.getInetAddress()+ \":\\n\"+ StringUtils.stringifyException(ioe));\n    throw ioe;\n  }\n finally {\n    IOUtils.closeStream(out);\n    IOUtils.closeStream(blockSender);\n  }\n}\n",
                "DataXceiver": "public DataXceiver(Socket s,DataNode datanode,DataXceiverServer dataXceiverServer){\n  this.s=s;\n  this.datanode=datanode;\n  this.dataXceiverServer=dataXceiverServer;\n  remoteAddress=s.getRemoteSocketAddress().toString();\n  localAddress=s.getLocalSocketAddress().toString();\n  LOG.debug(\"Number of active connections is: \" + datanode.getXceiverCount());\n}\n",
                "replaceBlock": "/** \n * Receive a block and write it to disk, it then notifies the namenode to remove the copy from the source.\n * @param in The stream to read from\n * @throws IOException\n */\nprivate void replaceBlock(DataInputStream in) throws IOException {\n  long blockId=in.readLong();\n  Block block=new Block(blockId,dataXceiverServer.estimateBlockSize,in.readLong());\n  String sourceID=Text.readString(in);\n  DatanodeInfo proxySource=new DatanodeInfo();\n  proxySource.readFields(in);\n  if (!dataXceiverServer.balanceThrottler.acquire()) {\n    LOG.warn(\"Not able to receive block \" + blockId + \" from \"+ s.getRemoteSocketAddress()+ \" because threads quota is exceeded.\");\n    sendResponse(s,(short)DataTransferProtocol.OP_STATUS_ERROR,datanode.socketWriteTimeout);\n    return;\n  }\n  Socket proxySock=null;\n  DataOutputStream proxyOut=null;\n  short opStatus=DataTransferProtocol.OP_STATUS_SUCCESS;\n  BlockReceiver blockReceiver=null;\n  DataInputStream proxyReply=null;\n  try {\n    InetSocketAddress proxyAddr=NetUtils.createSocketAddr(proxySource.getName());\n    proxySock=datanode.newSocket();\n    proxySock.connect(proxyAddr,datanode.socketTimeout);\n    proxySock.setSoTimeout(datanode.socketTimeout);\n    OutputStream baseStream=NetUtils.getOutputStream(proxySock,datanode.socketWriteTimeout);\n    proxyOut=new DataOutputStream(new BufferedOutputStream(baseStream,SMALL_BUFFER_SIZE));\n    proxyOut.writeShort(DataTransferProtocol.DATA_TRANSFER_VERSION);\n    proxyOut.writeByte(DataTransferProtocol.OP_COPY_BLOCK);\n    proxyOut.writeLong(block.getBlockId());\n    proxyOut.writeLong(block.getGenerationStamp());\n    proxyOut.flush();\n    proxyReply=new DataInputStream(new BufferedInputStream(NetUtils.getInputStream(proxySock),BUFFER_SIZE));\n    blockReceiver=new BlockReceiver(block,proxyReply,proxySock.getRemoteSocketAddress().toString(),proxySock.getLocalSocketAddress().toString(),false,\"\",null,datanode);\n    blockReceiver.receiveBlock(null,null,null,null,dataXceiverServer.balanceThrottler,-1);\n    datanode.notifyNamenodeReceivedBlock(block,sourceID);\n    LOG.info(\"Moved block \" + block + \" from \"+ s.getRemoteSocketAddress());\n  }\n catch (  IOException ioe) {\n    opStatus=DataTransferProtocol.OP_STATUS_ERROR;\n    throw ioe;\n  }\n finally {\n    if (opStatus == DataTransferProtocol.OP_STATUS_SUCCESS) {\n      try {\n        proxyReply.readChar();\n      }\n catch (      IOException ignored) {\n      }\n    }\n    dataXceiverServer.balanceThrottler.release();\n    try {\n      sendResponse(s,opStatus,datanode.socketWriteTimeout);\n    }\n catch (    IOException ioe) {\n      LOG.warn(\"Error writing reply back to \" + s.getRemoteSocketAddress());\n    }\n    IOUtils.closeStream(proxyOut);\n    IOUtils.closeStream(blockReceiver);\n    IOUtils.closeStream(proxyReply);\n  }\n}\n",
                "writeBlock": "/** \n * Write a block to disk.\n * @param in The stream to read from\n * @throws IOException\n */\nprivate void writeBlock(DataInputStream in) throws IOException {\n  DatanodeInfo srcDataNode=null;\n  LOG.debug(\"writeBlock receive buf size \" + s.getReceiveBufferSize() + \" tcp no delay \"+ s.getTcpNoDelay());\n  Block block=new Block(in.readLong(),dataXceiverServer.estimateBlockSize,in.readLong());\n  LOG.info(\"Receiving block \" + block + \" src: \"+ remoteAddress+ \" dest: \"+ localAddress);\n  int pipelineSize=in.readInt();\n  boolean isRecovery=in.readBoolean();\n  String client=Text.readString(in);\n  boolean hasSrcDataNode=in.readBoolean();\n  if (hasSrcDataNode) {\n    srcDataNode=new DatanodeInfo();\n    srcDataNode.readFields(in);\n  }\n  int numTargets=in.readInt();\n  if (numTargets < 0) {\n    throw new IOException(\"Mislabelled incoming datastream.\");\n  }\n  DatanodeInfo targets[]=new DatanodeInfo[numTargets];\n  for (int i=0; i < targets.length; i++) {\n    DatanodeInfo tmp=new DatanodeInfo();\n    tmp.readFields(in);\n    targets[i]=tmp;\n  }\n  DataOutputStream mirrorOut=null;\n  DataInputStream mirrorIn=null;\n  DataOutputStream replyOut=null;\n  Socket mirrorSock=null;\n  BlockReceiver blockReceiver=null;\n  String mirrorNode=null;\n  String firstBadLink=\"\";\n  try {\n    blockReceiver=new BlockReceiver(block,in,s.getRemoteSocketAddress().toString(),s.getLocalSocketAddress().toString(),isRecovery,client,srcDataNode,datanode);\n    replyOut=new DataOutputStream(NetUtils.getOutputStream(s,datanode.socketWriteTimeout));\n    if (targets.length > 0) {\n      InetSocketAddress mirrorTarget=null;\n      mirrorNode=targets[0].getName();\n      mirrorTarget=NetUtils.createSocketAddr(mirrorNode);\n      mirrorSock=datanode.newSocket();\n      try {\n        int timeoutValue=numTargets * datanode.socketTimeout;\n        int writeTimeout=datanode.socketWriteTimeout + (HdfsConstants.WRITE_TIMEOUT_EXTENSION * numTargets);\n        mirrorSock.connect(mirrorTarget,timeoutValue);\n        mirrorSock.setSoTimeout(timeoutValue);\n        mirrorSock.setSendBufferSize(DEFAULT_DATA_SOCKET_SIZE);\n        mirrorOut=new DataOutputStream(new BufferedOutputStream(NetUtils.getOutputStream(mirrorSock,writeTimeout),SMALL_BUFFER_SIZE));\n        mirrorIn=new DataInputStream(NetUtils.getInputStream(mirrorSock));\n        mirrorOut.writeShort(DataTransferProtocol.DATA_TRANSFER_VERSION);\n        mirrorOut.write(DataTransferProtocol.OP_WRITE_BLOCK);\n        mirrorOut.writeLong(block.getBlockId());\n        mirrorOut.writeLong(block.getGenerationStamp());\n        mirrorOut.writeInt(pipelineSize);\n        mirrorOut.writeBoolean(isRecovery);\n        Text.writeString(mirrorOut,client);\n        mirrorOut.writeBoolean(hasSrcDataNode);\n        if (hasSrcDataNode) {\n          srcDataNode.write(mirrorOut);\n        }\n        mirrorOut.writeInt(targets.length - 1);\n        for (int i=1; i < targets.length; i++) {\n          targets[i].write(mirrorOut);\n        }\n        blockReceiver.writeChecksumHeader(mirrorOut);\n        mirrorOut.flush();\n        if (client.length() != 0) {\n          firstBadLink=Text.readString(mirrorIn);\n          if (LOG.isDebugEnabled() || firstBadLink.length() > 0) {\n            LOG.info(\"Datanode \" + targets.length + \" got response for connect ack \"+ \" from downstream datanode with firstbadlink as \"+ firstBadLink);\n          }\n        }\n      }\n catch (      IOException e) {\n        if (client.length() != 0) {\n          Text.writeString(replyOut,mirrorNode);\n          replyOut.flush();\n        }\n        IOUtils.closeStream(mirrorOut);\n        mirrorOut=null;\n        IOUtils.closeStream(mirrorIn);\n        mirrorIn=null;\n        IOUtils.closeSocket(mirrorSock);\n        mirrorSock=null;\n        if (client.length() > 0) {\n          throw e;\n        }\n else {\n          LOG.info(datanode.dnRegistration + \":Exception transfering block \" + block+ \" to mirror \"+ mirrorNode+ \". continuing without the mirror.\\n\"+ StringUtils.stringifyException(e));\n        }\n      }\n    }\n    if (client.length() != 0) {\n      if (LOG.isDebugEnabled() || firstBadLink.length() > 0) {\n        LOG.info(\"Datanode \" + targets.length + \" forwarding connect ack to upstream firstbadlink is \"+ firstBadLink);\n      }\n      Text.writeString(replyOut,firstBadLink);\n      replyOut.flush();\n    }\n    String mirrorAddr=(mirrorSock == null) ? null : mirrorNode;\n    blockReceiver.receiveBlock(mirrorOut,mirrorIn,replyOut,mirrorAddr,null,targets.length);\n    if (client.length() == 0) {\n      datanode.notifyNamenodeReceivedBlock(block,DataNode.EMPTY_DEL_HINT);\n      LOG.info(\"Received block \" + block + \" src: \"+ remoteAddress+ \" dest: \"+ localAddress+ \" of size \"+ block.getNumBytes());\n    }\n    if (datanode.blockScanner != null) {\n      datanode.blockScanner.addBlock(block);\n    }\n  }\n catch (  IOException ioe) {\n    LOG.info(\"writeBlock \" + block + \" received exception \"+ ioe);\n    throw ioe;\n  }\n finally {\n    IOUtils.closeStream(mirrorOut);\n    IOUtils.closeStream(mirrorIn);\n    IOUtils.closeStream(replyOut);\n    IOUtils.closeSocket(mirrorSock);\n    IOUtils.closeStream(blockReceiver);\n  }\n}\n",
                "readMetadata": "/** \n * Reads the metadata and sends the data in one 'DATA_CHUNK'.\n * @param in\n */\nvoid readMetadata(DataInputStream in) throws IOException {\n  Block block=new Block(in.readLong(),0,in.readLong());\n  MetaDataInputStream checksumIn=null;\n  DataOutputStream out=null;\n  try {\n    checksumIn=datanode.data.getMetaDataInputStream(block);\n    long fileSize=checksumIn.getLength();\n    if (fileSize >= 1L << 31 || fileSize <= 0) {\n      throw new IOException(\"Unexpected size for checksumFile of block\" + block);\n    }\n    byte[] buf=new byte[(int)fileSize];\n    IOUtils.readFully(checksumIn,buf,0,buf.length);\n    out=new DataOutputStream(NetUtils.getOutputStream(s,datanode.socketWriteTimeout));\n    out.writeByte(DataTransferProtocol.OP_STATUS_SUCCESS);\n    out.writeInt(buf.length);\n    out.write(buf);\n    out.writeInt(0);\n  }\n  finally {\n    IOUtils.closeStream(out);\n    IOUtils.closeStream(checksumIn);\n  }\n}\n",
                "run": "/** \n * Read/write data from/to the DataXceiveServer.\n */\npublic void run(){\n  DataInputStream in=null;\n  try {\n    in=new DataInputStream(new BufferedInputStream(NetUtils.getInputStream(s),SMALL_BUFFER_SIZE));\n    short version=in.readShort();\n    if (version != DataTransferProtocol.DATA_TRANSFER_VERSION) {\n      throw new IOException(\"Version Mismatch\");\n    }\n    boolean local=s.getInetAddress().equals(s.getLocalAddress());\n    byte op=in.readByte();\n    int curXceiverCount=datanode.getXceiverCount();\n    if (curXceiverCount > dataXceiverServer.maxXceiverCount) {\n      throw new IOException(\"xceiverCount \" + curXceiverCount + \" exceeds the limit of concurrent xcievers \"+ dataXceiverServer.maxXceiverCount);\n    }\n    long startTime=DataNode.now();\nswitch (op) {\ncase DataTransferProtocol.OP_READ_BLOCK:\n      readBlock(in);\n    datanode.myMetrics.readBlockOp.inc(DataNode.now() - startTime);\n  if (local)   datanode.myMetrics.readsFromLocalClient.inc();\n else   datanode.myMetrics.readsFromRemoteClient.inc();\nbreak;\ncase DataTransferProtocol.OP_WRITE_BLOCK:\nwriteBlock(in);\ndatanode.myMetrics.writeBlockOp.inc(DataNode.now() - startTime);\nif (local) datanode.myMetrics.writesFromLocalClient.inc();\n else datanode.myMetrics.writesFromRemoteClient.inc();\nbreak;\ncase DataTransferProtocol.OP_READ_METADATA:\nreadMetadata(in);\ndatanode.myMetrics.readMetadataOp.inc(DataNode.now() - startTime);\nbreak;\ncase DataTransferProtocol.OP_REPLACE_BLOCK:\nreplaceBlock(in);\ndatanode.myMetrics.replaceBlockOp.inc(DataNode.now() - startTime);\nbreak;\ncase DataTransferProtocol.OP_COPY_BLOCK:\ncopyBlock(in);\ndatanode.myMetrics.copyBlockOp.inc(DataNode.now() - startTime);\nbreak;\ncase DataTransferProtocol.OP_BLOCK_CHECKSUM:\ngetBlockChecksum(in);\ndatanode.myMetrics.blockChecksumOp.inc(DataNode.now() - startTime);\nbreak;\ndefault :\nthrow new IOException(\"Unknown opcode \" + op + \" in data stream\");\n}\n}\n catch (Throwable t) {\nLOG.error(datanode.dnRegistration + \":DataXceiver\",t);\n}\n finally {\nLOG.debug(datanode.dnRegistration + \":Number of active connections is: \" + datanode.getXceiverCount());\nIOUtils.closeStream(in);\nIOUtils.closeSocket(s);\ndataXceiverServer.childSockets.remove(s);\n}\n}\n",
                "copyBlock": "/** \n * Read a block from the disk and then sends it to a destination.\n * @param in The stream to read from\n * @throws IOException\n */\nprivate void copyBlock(DataInputStream in) throws IOException {\n  long blockId=in.readLong();\n  Block block=new Block(blockId,0,in.readLong());\n  if (!dataXceiverServer.balanceThrottler.acquire()) {\n    LOG.info(\"Not able to copy block \" + blockId + \" to \"+ s.getRemoteSocketAddress()+ \" because threads quota is exceeded.\");\n    return;\n  }\n  BlockSender blockSender=null;\n  DataOutputStream reply=null;\n  boolean isOpSuccess=true;\n  try {\n    blockSender=new BlockSender(block,0,-1,false,false,false,datanode);\n    OutputStream baseStream=NetUtils.getOutputStream(s,datanode.socketWriteTimeout);\n    reply=new DataOutputStream(new BufferedOutputStream(baseStream,SMALL_BUFFER_SIZE));\n    long read=blockSender.sendBlock(reply,baseStream,dataXceiverServer.balanceThrottler);\n    datanode.myMetrics.bytesRead.inc((int)read);\n    datanode.myMetrics.blocksRead.inc();\n    LOG.info(\"Copied block \" + block + \" to \"+ s.getRemoteSocketAddress());\n  }\n catch (  IOException ioe) {\n    isOpSuccess=false;\n    throw ioe;\n  }\n finally {\n    dataXceiverServer.balanceThrottler.release();\n    if (isOpSuccess) {\n      try {\n        reply.writeChar('d');\n      }\n catch (      IOException ignored) {\n      }\n    }\n    IOUtils.closeStream(reply);\n    IOUtils.closeStream(blockSender);\n  }\n}\n",
                "sendResponse": "/** \n * Utility function for sending a response.\n * @param s socket to write to\n * @param opStatus status message to write\n * @param timeout send timeout\n */\nprivate void sendResponse(Socket s,short opStatus,long timeout) throws IOException {\n  DataOutputStream reply=new DataOutputStream(NetUtils.getOutputStream(s,timeout));\n  try {\n    reply.writeShort(opStatus);\n    reply.flush();\n  }\n  finally {\n    IOUtils.closeStream(reply);\n  }\n}\n"
            },
            "deps": [
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver Call org.apache.hadoop.hdfs.protocol.Block",
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver Import org.apache.hadoop.hdfs.protocol.Block",
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver Create org.apache.hadoop.hdfs.protocol.Block",
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver Contain org.apache.hadoop.hdfs.protocol.Block",
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver Import org.apache.hadoop.hdfs.protocol.FSConstants",
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver Use org.apache.hadoop.hdfs.protocol.FSConstants",
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver Implement org.apache.hadoop.hdfs.protocol.FSConstants",
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver Call org.apache.hadoop.hdfs.protocol.DatanodeID",
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver Call org.apache.hadoop.hdfs.protocol.DatanodeInfo",
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver Import org.apache.hadoop.hdfs.protocol.DatanodeInfo",
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver Use org.apache.hadoop.hdfs.protocol.DatanodeInfo",
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver Create org.apache.hadoop.hdfs.protocol.DatanodeInfo",
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver Contain org.apache.hadoop.hdfs.protocol.DatanodeInfo",
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver Import org.apache.hadoop.hdfs.server.common.HdfsConstants",
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver Use org.apache.hadoop.hdfs.server.common.HdfsConstants",
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver Call org.apache.hadoop.hdfs.server.datanode.BlockSender",
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver Use org.apache.hadoop.hdfs.server.datanode.BlockSender",
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver Create org.apache.hadoop.hdfs.server.datanode.BlockSender",
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver Contain org.apache.hadoop.hdfs.server.datanode.BlockSender",
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver Call org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader",
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver Use org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader",
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver Contain org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader",
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver Use org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeMetrics",
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver Call org.apache.hadoop.util.StringUtils",
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver Import org.apache.hadoop.util.StringUtils",
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver Use org.apache.hadoop.util.StringUtils",
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver Call org.apache.hadoop.metrics.util.MetricsLongValue",
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver Call org.apache.hadoop.metrics.util.MetricsTimeVaryingInt",
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver Call org.apache.hadoop.net.NetUtils",
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver Import org.apache.hadoop.net.NetUtils",
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver Use org.apache.hadoop.net.NetUtils",
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver Use org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration"
            ],
            "directory": "org.apache.hadoop.hdfs.server.datanode"
        }
    }
}
